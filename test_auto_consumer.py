#!/usr/bin/env python3
"""
Test du consumer Kafka Spark automatique avec vÃ©rification HDFS
"""

import requests
import time
import json
import subprocess
from datetime import datetime


def test_spark_auto_consumer():
    """Test complet du consumer automatique"""
    print("ðŸ§ª TEST DU CONSUMER KAFKA SPARK AUTOMATIQUE")
    print("=" * 60)

    # 1. VÃ©rifier que le container Spark est actif
    print("1ï¸âƒ£ VÃ©rification du container Spark...")
    try:
        result = subprocess.run(
            [
                "docker",
                "ps",
                "--filter",
                "name=tp5_planet-spark-app",
                "--format",
                "{{.Status}}",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )

        if "Up" in result.stdout:
            print("   âœ… Container Spark actif")
        else:
            print("   âŒ Container Spark non actif")
            return False
    except Exception as e:
        print(f"   âŒ Erreur: {e}")
        return False

    # 2. VÃ©rifier les logs rÃ©cents du consumer
    print("\n2ï¸âƒ£ VÃ©rification des logs du consumer...")
    try:
        result = subprocess.run(
            ["docker", "logs", "--tail", "10", "tp5_planet-spark-app"],
            capture_output=True,
            text=True,
            timeout=10,
        )

        logs = result.stdout
        if "DÃ‰MARRAGE PIPELINE" in logs or "Consumer Kafka" in logs:
            print("   âœ… Consumer en cours d'exÃ©cution")
        else:
            print("   âš ï¸  Consumer peut-Ãªtre en attente")

        # Afficher les derniÃ¨res lignes
        print("   ðŸ“‹ DerniÃ¨res lignes des logs:")
        for line in logs.split("\n")[-5:]:
            if line.strip():
                print(f"      {line}")

    except Exception as e:
        print(f"   âŒ Erreur logs: {e}")

    # 3. Envoyer une planÃ¨te de test
    print("\n3ï¸âƒ£ Envoi d'une planÃ¨te de test...")

    test_planet = {
        "Name": f"TestPlanet_{int(time.time())}",
        "Num_Moons": 3,
        "Minerals": 450,
        "Gravity": 9.8,
        "Sunlight_Hours": 12.5,
        "Temperature": 22.0,
        "Rotation_Time": 24.0,
        "Water_Presence": 1,
        "Colonisable": 1,
    }

    try:
        response = requests.post(
            "http://localhost:5001/discoveries", json=test_planet, timeout=10
        )

        if response.status_code == 201:
            print("   âœ… PlanÃ¨te envoyÃ©e avec succÃ¨s")
            print(f"   ðŸ“Š RÃ©ponse: {response.json()}")
        else:
            print(f"   âŒ Erreur envoi: {response.status_code}")
            return False

    except Exception as e:
        print(f"   âŒ Erreur requÃªte: {e}")
        return False

    # 4. Attendre et vÃ©rifier le traitement
    print("\n4ï¸âƒ£ Attente du traitement par Spark (30s)...")
    time.sleep(30)

    # 5. VÃ©rifier les nouveaux logs
    print("\n5ï¸âƒ£ VÃ©rification du traitement...")
    try:
        result = subprocess.run(
            ["docker", "logs", "--tail", "20", "tp5_planet-spark-app"],
            capture_output=True,
            text=True,
            timeout=10,
        )

        recent_logs = result.stdout

        # Chercher des signes de traitement
        if test_planet["Name"] in recent_logs:
            print("   âœ… PlanÃ¨te dÃ©tectÃ©e dans les logs Spark")
        elif "ANALYSE" in recent_logs or "RÃ©sultats sauvegardÃ©s" in recent_logs:
            print("   âœ… Traitement Spark dÃ©tectÃ©")
        else:
            print("   âš ï¸  Pas de trace Ã©vidente du traitement")

        print("   ðŸ“‹ Logs rÃ©cents du traitement:")
        for line in recent_logs.split("\n")[-10:]:
            if line.strip():
                print(f"      {line}")

    except Exception as e:
        print(f"   âŒ Erreur vÃ©rification: {e}")

    # 6. Statistiques du container
    print("\n6ï¸âƒ£ Statistiques du container...")
    try:
        result = subprocess.run(
            [
                "docker",
                "stats",
                "tp5_planet-spark-app",
                "--no-stream",
                "--format",
                "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}",
            ],
            capture_output=True,
            text=True,
            timeout=5,
        )

        print("   ðŸ“Š Utilisation des ressources:")
        print(f"      {result.stdout}")

    except Exception as e:
        print(f"   âŒ Erreur stats: {e}")

    # 7. VÃ©rification HDFS
    print("\n7ï¸âƒ£ VÃ©rification de la sauvegarde HDFS...")
    hdfs_verified = verify_hdfs_storage(test_planet["Name"])

    if hdfs_verified:
        print("   âœ… DonnÃ©es sauvegardÃ©es dans HDFS")
    else:
        print("   âš ï¸  Sauvegarde HDFS non vÃ©rifiÃ©e")

    # 8. Statistiques HDFS
    print("\n8ï¸âƒ£ Statistiques HDFS...")
    get_hdfs_statistics()

    print(f"\nâ° Test terminÃ© Ã  {datetime.now().strftime('%H:%M:%S')}")
    print("ðŸŽ¯ Le consumer Kafka Spark automatique est opÃ©rationnel !")

    if hdfs_verified:
        print("ðŸ’¾ SystÃ¨me complet : Kafka â†’ Spark â†’ HDFS âœ…")

    return True


def verify_hdfs_storage(planet_name):
    """VÃ©rifier que les donnÃ©es de la planÃ¨te sont dans HDFS"""
    try:
        print("   ðŸ” Recherche de la planÃ¨te dans HDFS...")

        # CrÃ©er un script Spark temporaire pour vÃ©rifier HDFS
        verification_script = f"""
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("VerifyHDFS").getOrCreate()
spark.sparkContext.setLogLevel("WARN")

try:
    # VÃ©rifier les donnÃ©es brutes
    raw_data = spark.read.parquet("hdfs://namenode:9000/planet_analytics/raw_data/planets_*")
    planet_found = raw_data.filter(raw_data.Name == "{planet_name}").count()
    
    print(f"HDFS_CHECK:RAW_DATA:{{planet_found}}")
    
    # VÃ©rifier les analyses d'habitabilitÃ©
    try:
        habitability = spark.read.parquet("hdfs://namenode:9000/planet_analytics/results/habitability_*")
        hab_found = habitability.filter(habitability.Name == "{planet_name}").count()
        print(f"HDFS_CHECK:HABITABILITY:{{hab_found}}")
    except:
        print("HDFS_CHECK:HABITABILITY:0")
    
    # VÃ©rifier les analyses de colonisation
    try:
        colonisation = spark.read.parquet("hdfs://namenode:9000/planet_analytics/results/colonisation_*")
        col_found = colonisation.filter(colonisation.Name == "{planet_name}").count()
        print(f"HDFS_CHECK:COLONISATION:{{col_found}}")
    except:
        print("HDFS_CHECK:COLONISATION:0")
        
except Exception as e:
    print(f"HDFS_CHECK:ERROR:{{e}}")
finally:
    spark.stop()
"""

        # Ã‰crire le script temporaire
        with open("/tmp/verify_hdfs.py", "w") as f:
            f.write(verification_script)

        # Copier dans le container Spark
        subprocess.run(
            [
                "docker",
                "cp",
                "/tmp/verify_hdfs.py",
                "tp5_planet-spark-app:/tmp/verify_hdfs.py",
            ],
            timeout=10,
        )

        # ExÃ©cuter la vÃ©rification
        result = subprocess.run(
            [
                "docker",
                "exec",
                "tp5_planet-spark-app",
                "/spark/bin/spark-submit",
                "--packages",
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0",
                "/tmp/verify_hdfs.py",
            ],
            capture_output=True,
            text=True,
            timeout=60,
        )

        # Analyser les rÃ©sultats
        output = result.stdout

        raw_count = 0
        hab_count = 0
        col_count = 0

        for line in output.split("\n"):
            if line.startswith("HDFS_CHECK:"):
                parts = line.split(":")
                if len(parts) >= 3:
                    check_type = parts[1]
                    count = parts[2]

                    try:
                        count_val = int(count)
                        if check_type == "RAW_DATA":
                            raw_count = count_val
                        elif check_type == "HABITABILITY":
                            hab_count = count_val
                        elif check_type == "COLONISATION":
                            col_count = count_val
                    except ValueError:
                        if check_type == "ERROR":
                            print(f"   âŒ Erreur HDFS: {count}")

        print(f"   ðŸ“Š RÃ©sultats de vÃ©rification HDFS:")
        print(f"      â€¢ DonnÃ©es brutes: {raw_count} enregistrement(s)")
        print(f"      â€¢ Analyses habitabilitÃ©: {hab_count} enregistrement(s)")
        print(f"      â€¢ Analyses colonisation: {col_count} enregistrement(s)")

        # Nettoyage
        subprocess.run(["rm", "-f", "/tmp/verify_hdfs.py"])

        return raw_count > 0 and (hab_count > 0 or col_count > 0)

    except Exception as e:
        print(f"   âŒ Erreur vÃ©rification HDFS: {e}")
        return False


def get_hdfs_statistics():
    """Afficher les statistiques HDFS"""
    try:
        print("   ðŸ“ Structure des rÃ©pertoires HDFS:")

        # Lister les rÃ©pertoires HDFS
        result = subprocess.run(
            [
                "docker",
                "exec",
                "tp5_planet-namenode",
                "hdfs",
                "dfs",
                "-ls",
                "-R",
                "/planet_analytics",
            ],
            capture_output=True,
            text=True,
            timeout=15,
        )

        if result.returncode == 0:
            lines = result.stdout.strip().split("\n")

            file_count = 0
            total_size = 0

            for line in lines:
                if line.startswith("d") or line.startswith("-"):
                    parts = line.split()
                    if len(parts) >= 5:
                        size = parts[4]
                        path = parts[-1]

                        if not line.startswith("d"):  # C'est un fichier
                            file_count += 1
                            try:
                                total_size += int(size)
                            except ValueError:
                                pass

                        # Afficher les Ã©lÃ©ments principaux
                        if "/planet_analytics/" in path:
                            item_type = "ðŸ“" if line.startswith("d") else "ðŸ“„"
                            size_display = (
                                f"({size} bytes)" if not line.startswith("d") else ""
                            )
                            print(
                                f"      {item_type} {path.split('/')[-1]} {size_display}"
                            )

            print(f"   ðŸ“Š Statistiques HDFS:")
            print(f"      â€¢ Nombre de fichiers: {file_count}")
            print(f"      â€¢ Taille totale: {total_size:,} bytes")

        else:
            print("   âš ï¸  Impossible d'accÃ©der Ã  HDFS")

    except Exception as e:
        print(f"   âŒ Erreur statistiques HDFS: {e}")


def show_planet_from_hdfs(planet_name):
    """Afficher une planÃ¨te spÃ©cifique depuis HDFS"""
    try:
        print(f"ðŸ” RECHERCHE DE '{planet_name}' DANS HDFS")
        print("=" * 50)

        # Script pour lire depuis HDFS
        read_script = f"""
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ReadPlanetFromHDFS").getOrCreate()
spark.sparkContext.setLogLevel("WARN")

try:
    # Lire les donnÃ©es brutes
    raw_data = spark.read.parquet("hdfs://namenode:9000/planet_analytics/raw_data/planets_*")
    planet_data = raw_data.filter(raw_data.Name == "{planet_name}")
    
    if planet_data.count() > 0:
        print("\\nðŸŒ DONNÃ‰ES BRUTES:")
        planet_data.show(truncate=False)
        
        # Lire les analyses
        try:
            habitability = spark.read.parquet("hdfs://namenode:9000/planet_analytics/results/habitability_*")
            planet_hab = habitability.filter(habitability.Name == "{planet_name}")
            
            if planet_hab.count() > 0:
                print("\\nðŸŒ± ANALYSE D'HABITABILITÃ‰:")
                planet_hab.select("Name", "conditions_habitables", "Temperature", "Gravity", "Water_Presence").show(truncate=False)
        except:
            print("\\nâš ï¸  Pas d'analyse d'habitabilitÃ© trouvÃ©e")
            
        try:
            colonisation = spark.read.parquet("hdfs://namenode:9000/planet_analytics/results/colonisation_*")
            planet_col = colonisation.filter(colonisation.Name == "{planet_name}")
            
            if planet_col.count() > 0:
                print("\\nðŸš€ ANALYSE DE COLONISATION:")
                planet_col.select("Name", "score_colonisation", "potentiel_colonisation", "Temperature", "Gravity").show(truncate=False)
        except:
            print("\\nâš ï¸  Pas d'analyse de colonisation trouvÃ©e")
    else:
        print(f"\\nâŒ PlanÃ¨te '{planet_name}' non trouvÃ©e dans HDFS")
        
except Exception as e:
    print(f"\\nâŒ Erreur: {{e}}")
finally:
    spark.stop()
"""

        # Ã‰crire et exÃ©cuter le script
        with open("/tmp/read_planet.py", "w") as f:
            f.write(read_script)

        subprocess.run(
            [
                "docker",
                "cp",
                "/tmp/read_planet.py",
                "tp5_planet-spark-app:/tmp/read_planet.py",
            ],
            timeout=10,
        )

        result = subprocess.run(
            [
                "docker",
                "exec",
                "tp5_planet-spark-app",
                "/spark/bin/spark-submit",
                "--packages",
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0",
                "/tmp/read_planet.py",
            ],
            timeout=60,
        )

        # Nettoyage
        subprocess.run(["rm", "-f", "/tmp/read_planet.py"])

    except Exception as e:
        print(f"âŒ Erreur lecture HDFS: {e}")


def monitor_consumer_realtime():
    """Monitoring en temps rÃ©el du consumer"""
    print("ðŸ“¡ MONITORING TEMPS RÃ‰EL DU CONSUMER")
    print("=" * 50)
    print("Appuyez sur Ctrl+C pour arrÃªter")

    try:
        while True:
            # Afficher les logs en temps rÃ©el
            result = subprocess.run(
                ["docker", "logs", "--tail", "5", "tp5_planet-spark-app"],
                capture_output=True,
                text=True,
                timeout=5,
            )

            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"\n[{timestamp}] Logs rÃ©cents:")

            for line in result.stdout.split("\n")[-3:]:
                if line.strip():
                    print(f"   {line}")

            time.sleep(10)

    except KeyboardInterrupt:
        print("\nðŸ‘‹ Monitoring arrÃªtÃ©")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        command = sys.argv[1]

        if command == "monitor":
            monitor_consumer_realtime()
        elif command == "hdfs":
            if len(sys.argv) > 2:
                planet_name = sys.argv[2]
                show_planet_from_hdfs(planet_name)
            else:
                print("Usage: python test_auto_consumer.py hdfs <nom_planete>")
        elif command == "stats":
            print("ðŸ“Š STATISTIQUES HDFS")
            print("=" * 30)
            get_hdfs_statistics()
        else:
            print("Commandes disponibles:")
            print("  python test_auto_consumer.py          # Test complet")
            print("  python test_auto_consumer.py monitor  # Monitoring temps rÃ©el")
            print(
                "  python test_auto_consumer.py hdfs <nom>  # Lire planÃ¨te depuis HDFS"
            )
            print("  python test_auto_consumer.py stats    # Statistiques HDFS")
    else:
        test_spark_auto_consumer()
