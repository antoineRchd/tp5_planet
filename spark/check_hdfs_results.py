#!/usr/bin/env python3
"""
Script pour v√©rifier et afficher les sauvegardes HDFS des analyses de plan√®tes
"""

from pyspark.sql import SparkSession
import os


def main():
    print("üîç V√âRIFICATION DES SAUVEGARDES HDFS")
    print("=" * 50)

    # Configuration Spark
    spark = SparkSession.builder.appName("CheckHDFSResults").getOrCreate()
    spark.sparkContext.setLogLevel("WARN")

    try:
        # Lister les fichiers dans HDFS
        print("\nüìÅ Structure des r√©pertoires HDFS:")

        # 1. M√©tadonn√©es disponibles
        try:
            metadata_files = spark.read.parquet(
                "hdfs://namenode:9000/planet_analytics/metadata_*"
            )
            print("\nüìã M√©tadonn√©es d'analyses disponibles:")
            metadata_files.select(
                "timestamp", "nb_planetes", "type_analyse", "description"
            ).show(truncate=False)
        except Exception as e:
            print("‚ö†Ô∏è Aucune m√©tadonn√©e trouv√©e: {}".format(e))

        # 2. Donn√©es brutes les plus r√©centes
        try:
            latest_raw = spark.read.parquet(
                "hdfs://namenode:9000/planet_analytics/raw_data/planets_*"
            )
            print(f"\nüåç Donn√©es brutes - Nombre de plan√®tes: {latest_raw.count()}")
            print("Colonnes disponibles:", latest_raw.columns)
            latest_raw.show(5)
        except Exception as e:
            print("‚ö†Ô∏è Aucune donn√©e brute trouv√©e: {}".format(e))

        # 3. Statistiques de base les plus r√©centes
        try:
            latest_stats = spark.read.parquet(
                "hdfs://namenode:9000/planet_analytics/stats/basic_stats_*"
            )
            print("\nüìä Statistiques de base:")
            latest_stats.show()
        except Exception as e:
            print("‚ö†Ô∏è Aucune statistique trouv√©e: {}".format(e))

        # 4. Analyse d'habitabilit√©
        try:
            habitability = spark.read.parquet(
                "hdfs://namenode:9000/planet_analytics/results/habitability_*"
            )
            print(
                f"\nüåç Analyse d'habitabilit√© - Nombre de plan√®tes: {habitability.count()}"
            )
            habitability.groupBy("conditions_habitables").count().show()
        except Exception as e:
            print("‚ö†Ô∏è Aucune analyse d'habitabilit√© trouv√©e: {}".format(e))

        # 5. Analyse de colonisation
        try:
            colonisation = spark.read.parquet(
                "hdfs://namenode:9000/planet_analytics/results/colonisation_*"
            )
            print(
                f"\nüöÄ Analyse de colonisation - Nombre de plan√®tes: {colonisation.count()}"
            )
            colonisation.groupBy("potentiel_colonisation").count().show()
        except Exception as e:
            print("‚ö†Ô∏è Aucune analyse de colonisation trouv√©e: {}".format(e))

        # 6. Top 10 de colonisation
        try:
            top10 = spark.read.parquet(
                "hdfs://namenode:9000/planet_analytics/results/top10_colonisation_*"
            )
            print(f"\nüèÜ Top 10 de colonisation:")
            top10.select(
                "Name",
                "score_colonisation",
                "potentiel_colonisation",
                "Temperature",
                "Gravity",
            ).show()
        except Exception as e:
            print("‚ö†Ô∏è Aucun top 10 trouv√©: {}".format(e))

        print("\n‚úÖ V√âRIFICATION TERMIN√âE")

    except Exception as e:
        print("‚ùå Erreur lors de la v√©rification: {}".format(e))
        import traceback

        traceback.print_exc()
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
