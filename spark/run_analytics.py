#!/usr/bin/env python3
"""
Script d'orchestration pour les analyses avanc√©es et mod√®les d'IA
Utilise la nouvelle structure de donn√©es des plan√®tes
"""

import os
import sys
import time
from subprocess import run, PIPE
import json


def run_command(command, description):
    """
    Ex√©cute une commande et affiche les r√©sultats
    """
    print(f"\n{'='*60}")
    print(f"üöÄ {description}")
    print(f"{'='*60}")

    try:
        result = run(command, shell=True, capture_output=True, text=True, timeout=1800)

        if result.returncode == 0:
            print(f"‚úÖ {description} - Succ√®s")
            if result.stdout:
                print("üìÑ Output:")
                print(result.stdout)
        else:
            print(f"‚ùå {description} - Erreur (code: {result.returncode})")
            if result.stderr:
                print("üö® Erreur:")
                print(result.stderr)

        return result.returncode == 0

    except Exception as e:
        print(f"‚ùå Erreur lors de l'ex√©cution: {e}")
        return False


def check_spark_environment():
    """
    V√©rifie que l'environnement Spark est correctement configur√©
    """
    print("üîç V√©rification de l'environnement Spark...")

    # Variables d'environnement importantes
    env_vars = ["SPARK_MASTER_URL", "KAFKA_BOOTSTRAP_SERVERS", "HDFS_NAMENODE"]

    for var in env_vars:
        value = os.getenv(var)
        if value:
            print(f"  ‚úÖ {var}: {value}")
        else:
            print(f"  ‚ö†Ô∏è {var}: Non d√©fini")

    # Test de connectivit√© Spark
    spark_master = os.getenv("SPARK_MASTER_URL", "spark://spark-master:7077")
    print(f"\nüîó Test de connectivit√© Spark Master: {spark_master}")


def run_advanced_analytics():
    """
    Lance les analyses avanc√©es
    """
    print("\nüî¨ LANCEMENT DES ANALYSES AVANC√âES")

    command = """
    spark-submit \
        --master $SPARK_MASTER_URL \
        --deploy-mode client \
        --driver-memory 2g \
        --executor-memory 2g \
        --executor-cores 2 \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.sql.adaptive.coalescePartitions.enabled=true \
        /app/advanced_analytics.py
    """

    return run_command(command, "Analyses Avanc√©es des Plan√®tes")


def run_habitability_predictor():
    """
    Lance le mod√®le de pr√©diction d'habitabilit√©
    """
    print("\nü§ñ LANCEMENT DU MOD√àLE D'HABITABILIT√â")

    command = """
    spark-submit \
        --master $SPARK_MASTER_URL \
        --deploy-mode client \
        --driver-memory 4g \
        --executor-memory 4g \
        --executor-cores 2 \
        --conf spark.sql.adaptive.enabled=true \
        --conf spark.sql.adaptive.coalescePartitions.enabled=true \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0 \
        /app/habitability_predictor.py
    """

    return run_command(command, "Mod√®le de Pr√©diction d'Habitabilit√©")


def run_kafka_integration():
    """
    Lance l'int√©gration avec Kafka pour traiter les donn√©es en temps r√©el
    """
    print("\nüì° LANCEMENT DE L'INT√âGRATION KAFKA")

    command = """
    spark-submit \
        --master $SPARK_MASTER_URL \
        --deploy-mode client \
        --driver-memory 2g \
        --executor-memory 2g \
        --executor-cores 2 \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0 \
        /app/kafka_spark_processor.py
    """

    return run_command(command, "Int√©gration Kafka-Spark")


def display_summary():
    """
    Affiche un r√©sum√© des analyses effectu√©es
    """
    print("\n" + "=" * 60)
    print("üìä R√âSUM√â DES ANALYSES TERMIN√âES")
    print("=" * 60)

    print("\nüî¨ Analyses Avanc√©es:")
    print("  ‚Ä¢ Statistiques descriptives des plan√®tes")
    print("  ‚Ä¢ Analyse des corr√©lations entre caract√©ristiques")
    print("  ‚Ä¢ Relation pr√©sence d'eau vs autres facteurs")
    print("  ‚Ä¢ Clustering des plan√®tes similaires")
    print("  ‚Ä¢ D√©tection d'anomalies")
    print("  ‚Ä¢ Analyse des facteurs d'habitabilit√©")

    print("\nü§ñ Mod√®le d'IA - Pr√©diction d'Habitabilit√©:")
    print("  ‚Ä¢ Entra√Ænement de mod√®les ML (RandomForest, GBT, LogisticRegression)")
    print("  ‚Ä¢ Ing√©nierie de features avanc√©es")
    print("  ‚Ä¢ √âvaluation de performance (AUC, Accuracy, F1-Score)")
    print("  ‚Ä¢ Analyse d'importance des features")
    print("  ‚Ä¢ Validation crois√©e pour robustesse")
    print("  ‚Ä¢ Pr√©dictions sur nouvelles plan√®tes")

    print("\nüíæ Donn√©es sauvegard√©es:")
    print("  ‚Ä¢ HDFS: /planet_analytics/ et /planet_ml_models/")
    print("  ‚Ä¢ Hive: Tables planet_discoveries.raw_data et clustered_data")
    print("  ‚Ä¢ Mod√®le ML persist√© pour r√©utilisation")

    print("\nüìà Facteurs cl√©s identifi√©s pour l'habitabilit√©:")
    print("  ‚Ä¢ Zone de temp√©rature habitable (-50¬∞C √† 50¬∞C)")
    print("  ‚Ä¢ Gravit√© terrestre (0.5 √† 2.0)")
    print("  ‚Ä¢ Pr√©sence d'eau")
    print("  ‚Ä¢ Exposition solaire mod√©r√©e (8-16h)")
    print("  ‚Ä¢ Rotation mod√©r√©e (12-48h)")
    print("  ‚Ä¢ Richesse min√©rale √©quilibr√©e")


def main():
    """
    Fonction principale d'orchestration
    """
    print("üåç D√âMARRAGE DU PIPELINE D'ANALYSE PLAN√âTAIRE")
    print("=" * 60)
    print("üîÑ Utilise la nouvelle structure de donn√©es:")
    print("   Name, Num_Moons, Minerals, Gravity, Sunlight_Hours,")
    print("   Temperature, Rotation_Time, Water_Presence, Colonisable")
    print("=" * 60)

    start_time = time.time()

    # √âtape 1: V√©rification de l'environnement
    check_spark_environment()

    # √âtape 2: Analyses avanc√©es
    analytics_success = run_advanced_analytics()

    # √âtape 3: Mod√®le d'habitabilit√©
    if analytics_success:
        predictor_success = run_habitability_predictor()
    else:
        print("‚ö†Ô∏è Analyses avanc√©es √©chou√©es, saut du mod√®le d'habitabilit√©")
        predictor_success = False

    # √âtape 4: Int√©gration Kafka (optionnelle)
    if analytics_success and predictor_success:
        print("\nüîÑ Int√©gration Kafka disponible mais non lanc√©e automatiquement")
        print("   Pour l'activer: python /app/kafka_spark_processor.py")

    # R√©sum√© final
    end_time = time.time()
    duration = end_time - start_time

    print(f"\n‚è±Ô∏è Dur√©e totale d'ex√©cution: {duration:.1f} secondes")

    if analytics_success and predictor_success:
        print("‚úÖ PIPELINE COMPL√âT√â AVEC SUCC√àS!")
        display_summary()
    else:
        print("‚ùå PIPELINE PARTIELLEMENT √âCHOU√â")
        if not analytics_success:
            print("  ‚Ä¢ Analyses avanc√©es: √âCHEC")
        if not predictor_success:
            print("  ‚Ä¢ Mod√®le d'habitabilit√©: √âCHEC")

    return analytics_success and predictor_success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
