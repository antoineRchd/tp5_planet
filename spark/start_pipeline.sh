#!/bin/bash

echo "ğŸŒ DÃ‰MARRAGE PIPELINE KAFKA SPARK PLANÃˆTES"
echo "============================================="

# Fonction d'attente des services
wait_for_service() {
    local service=$1
    local port=$2
    local name=$3
    
    echo "ğŸ” VÃ©rification de $name..."
    until nc -z $service $port; do
        echo "   $name non disponible, attente 5s..."
        sleep 5
    done
    echo "âœ… $name connectÃ©"
}

# Attendre que les services soient prÃªts
echo "â³ Initialisation (30s)..."
sleep 30

# VÃ©rifier tous les services
wait_for_service kafka 29092 "Kafka"
wait_for_service spark-master 7077 "Spark Master" 
wait_for_service namenode 9000 "HDFS"

echo "ğŸ¯ Tous les services sont prÃªts !"

# DÃ©tecter le mode Ã  partir de la variable d'environnement
MODE=${SPARK_MODE:-"consumer"}

echo "ğŸ“Š Mode sÃ©lectionnÃ©: $MODE"
echo "============================================="

case $MODE in
    "consumer")
        echo "ğŸ“¡ Lancement du Consumer Kafka temps rÃ©el..."
        exec /spark/bin/spark-submit \
            --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
            --master spark://spark-master:7077 \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            --total-executor-cores 2 \
            --conf spark.sql.adaptive.enabled=false \
            --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
            /app/kafka_consumer_simple.py
        ;;
    
    "analytics")
        echo "ğŸ“Š Lancement des Analytics avancÃ©es..."
        exec /spark/bin/spark-submit \
            --master spark://spark-master:7077 \
            --driver-memory 2g \
            --executor-memory 2g \
            --executor-cores 2 \
            --total-executor-cores 4 \
            /app/advanced_analytics.py
        ;;
    
    "ml")
        echo "ğŸ¤– Lancement du modÃ¨le ML..."
        exec /spark/bin/spark-submit \
            --master spark://spark-master:7077 \
            --driver-memory 2g \
            --executor-memory 2g \
            --executor-cores 2 \
            --total-executor-cores 4 \
            /app/habitability_predictor.py
        ;;
    
    "pipeline")
        echo "ğŸ”„ Lancement du pipeline complet..."
        exec /spark/bin/spark-submit \
            --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
            --master spark://spark-master:7077 \
            --driver-memory 2g \
            --executor-memory 2g \
            --executor-cores 2 \
            --total-executor-cores 4 \
            /app/main_pipeline.py
        ;;
    
    "interactive")
        echo "ğŸ’» Mode interactif - gardez le container actif..."
        echo "Utilisez docker exec pour lancer manuellement les scripts"
        exec tail -f /dev/null
        ;;
    
    *)
        echo "âŒ Mode inconnu: $MODE"
        echo "Modes disponibles: consumer, analytics, ml, pipeline, interactive"
        echo "ğŸ”„ DÃ©marrage du consumer par dÃ©faut..."
        exec /spark/bin/spark-submit \
            --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
            --master spark://spark-master:7077 \
            --driver-memory 1g \
            --executor-memory 1g \
            /app/kafka_consumer_simple.py
        ;;
esac 