from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json
import os


def create_spark_session():
    """
    Cr√©e une session Spark configur√©e pour Kafka, HDFS et Hive
    """
    spark = (
        SparkSession.builder.appName("PlanetDiscoveryProcessor")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config(
            "spark.jars.packages",
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,"
            "org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0",
        )
        .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoints")
        .config("spark.sql.warehouse.dir", "/spark-warehouse")
        .enableHiveSupport()
        .getOrCreate()
    )

    spark.sparkContext.setLogLevel("WARN")
    return spark


def define_planet_schema():
    """
    D√©finit le sch√©ma pour les donn√©es de d√©couverte de plan√®tes
    """
    return StructType(
        [
            StructField("id", StringType(), True),
            StructField("nom", StringType(), True),
            StructField("decouvreur", StringType(), True),
            StructField("date_decouverte", StringType(), True),
            StructField("masse", DoubleType(), True),
            StructField("rayon", DoubleType(), True),
            StructField("distance", DoubleType(), True),
            StructField("type", StringType(), True),
            StructField("statut", StringType(), True),
            StructField("atmosphere", StringType(), True),
            StructField("temperature_moyenne", DoubleType(), True),
            StructField("periode_orbitale", DoubleType(), True),
            StructField("nombre_satellites", IntegerType(), True),
            StructField("presence_eau", StringType(), True),
            StructField("timestamp_reception", StringType(), True),
        ]
    )


def read_kafka_stream(spark, kafka_servers, topic):
    """
    Lit le stream Kafka et parse les donn√©es JSON
    """
    df = (
        spark.readStream.format("kafka")
        .option("kafka.bootstrap.servers", kafka_servers)
        .option("subscribe", topic)
        .option("startingOffsets", "earliest")
        .load()
    )

    # Parse JSON data
    planet_schema = define_planet_schema()

    parsed_df = df.select(
        col("key").cast("string"),
        from_json(col("value").cast("string"), planet_schema).alias("data"),
        col("timestamp").alias("kafka_timestamp"),
    ).select("data.*", "kafka_timestamp")

    return parsed_df


def process_realtime_analytics(df):
    """
    Traitement en temps r√©el des donn√©es
    """
    # Agr√©gations en temps r√©el
    aggregated_df = (
        df.withWatermark("kafka_timestamp", "10 minutes")
        .groupBy(
            window(col("kafka_timestamp"), "5 minutes"), col("type"), col("statut")
        )
        .agg(
            count("*").alias("count"),
            avg("masse").alias("avg_masse"),
            avg("rayon").alias("avg_rayon"),
            avg("distance").alias("avg_distance"),
            avg("temperature_moyenne").alias("avg_temperature"),
        )
    )

    return aggregated_df


def write_to_hdfs(df, path, mode="append"):
    """
    √âcrit les donn√©es vers HDFS
    """
    df.write.mode(mode).option("path", path).format("parquet").save()


def write_to_hive(df, table_name, mode="append"):
    """
    √âcrit les donn√©es vers Hive
    """
    df.write.mode(mode).saveAsTable(table_name)


def main():
    """
    Fonction principale de traitement des donn√©es
    """
    print("üöÄ D√©marrage du processeur de d√©couvertes de plan√®tes")

    # Configuration
    kafka_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")
    topic = "planet_discoveries"
    hdfs_namenode = os.getenv("HDFS_NAMENODE", "hdfs://namenode:9000")

    # Cr√©ation de la session Spark
    spark = create_spark_session()

    try:
        # Lecture du stream Kafka
        print(f"üì° Connexion √† Kafka: {kafka_servers}")
        print(f"üìä Topic: {topic}")

        df = read_kafka_stream(spark, kafka_servers, topic)

        # Affichage de la structure des donn√©es
        df.printSchema()

        # Traitement et affichage en temps r√©el
        query = (
            df.writeStream.outputMode("append")
            .format("console")
            .option("truncate", False)
            .trigger(processingTime="30 seconds")
            .start()
        )

        # Sauvegarde vers HDFS
        hdfs_query = (
            df.writeStream.outputMode("append")
            .format("parquet")
            .option("path", f"{hdfs_namenode}/planet_discoveries/raw")
            .option("checkpointLocation", "/tmp/checkpoints/hdfs")
            .trigger(processingTime="60 seconds")
            .start()
        )

        # Agr√©gations en temps r√©el
        aggregated_df = process_realtime_analytics(df)

        aggregated_query = (
            aggregated_df.writeStream.outputMode("update")
            .format("console")
            .option("truncate", False)
            .trigger(processingTime="60 seconds")
            .start()
        )

        print("‚úÖ Streaming d√©marr√©. Appuyez sur Ctrl+C pour arr√™ter...")

        # Attente des queries
        query.awaitTermination()
        hdfs_query.awaitTermination()
        aggregated_query.awaitTermination()

    except Exception as e:
        print(f"‚ùå Erreur: {e}")
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
