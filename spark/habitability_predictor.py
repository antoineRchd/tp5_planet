from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.feature import (
    VectorAssembler,
    StandardScaler,
    StringIndexer,
    OneHotEncoder,
)
from pyspark.ml.classification import (
    RandomForestClassifier,
    GBTClassifier,
    LogisticRegression,
)
from pyspark.ml.evaluation import (
    BinaryClassificationEvaluator,
    MulticlassClassificationEvaluator,
)
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor
import os
import numpy as np


def create_spark_session():
    """
    CrÃ©e une session Spark pour le machine learning
    """
    spark = (
        SparkSession.builder.appName("PlanetHabitabilityPredictor")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.sql.warehouse.dir", "/spark-warehouse")
        .enableHiveSupport()
        .getOrCreate()
    )

    spark.sparkContext.setLogLevel("WARN")
    return spark


def create_enhanced_dataset(spark):
    """
    CrÃ©e un dataset enrichi avec plus d'exemples pour l'entraÃ®nement
    Utilise la nouvelle structure: Name, Num_Moons, Minerals, Gravity, Sunlight_Hours, Temperature, Rotation_Time, Water_Presence, Colonisable
    """
    # Dataset Ã©tendu avec des planÃ¨tes et leurs caractÃ©ristiques d'habitabilitÃ©
    enhanced_data = [
        # PlanÃ¨tes habitables/colonisables
        ("Kepler-442b", 0, 45, 0.89, 12.5, -40.0, 112.3, 0, 1),
        ("Kepler-452b", 1, 78, 1.2, 16.8, 5.0, 385.0, 0, 1),
        ("HD_40307g", 2, 89, 1.8, 18.2, 15.0, 197.8, 1, 1),
        ("Proxima_Centauri_b", 0, 23, 1.1, 11.0, -39.0, 11.2, 0, 1),
        ("TRAPPIST-1e", 0, 34, 0.92, 8.5, -22.0, 6.1, 1, 1),
        ("Gliese_667Cc", 0, 56, 1.5, 13.5, -3.0, 28.1, 1, 1),
        ("K2-18b", 0, 67, 2.3, 20.1, -23.0, 33.0, 1, 1),
        ("TOI-715b", 0, 45, 1.55, 15.2, 15.0, 19.3, 0, 1),
        ("LP_890-9c", 0, 38, 1.4, 12.8, -25.0, 8.8, 0, 1),
        ("GJ_357d", 0, 52, 1.7, 17.3, -53.0, 55.7, 0, 1),
        ("Ocean_World_1", 3, 67, 1.3, 16.8, 8.0, 78.2, 1, 1),
        ("Kepler-1649c", 0, 41, 1.06, 10.5, -39.0, 19.5, 0, 1),
        ("TOI-175b", 0, 73, 1.73, 18.6, 8.0, 25.6, 0, 1),
        ("LHS_1140b", 0, 58, 1.4, 14.2, -53.0, 24.7, 0, 1),
        # PlanÃ¨tes tempÃ©rÃ©es mais non habitables (facteurs dÃ©favorables)
        ("Planet_18329", 5, 59, 1.98, 5.8, 28.4, 56.8, 0, 0),
        ("Planet_28900", 8, 672, 1.39, 14.7, 27.5, 51.0, 0, 0),
        ("Desert_Planet_1", 2, 890, 2.1, 18.5, 85.0, 45.6, 0, 0),
        ("Rocky_Planet_1", 1, 423, 1.9, 14.2, 120.0, 89.4, 0, 0),
        # PlanÃ¨tes trop chaudes
        ("Hot_Jupiter_1", 15, 12, 0.8, 24.0, 1200.0, 3.2, 0, 0),
        ("Venus_Like_1", 0, 234, 0.9, 22.3, 462.0, 243.0, 0, 0),
        ("Volcanic_Planet_1", 0, 789, 2.8, 19.6, 450.0, 156.7, 0, 0),
        ("Mercury_Like_1", 0, 156, 0.38, 23.8, 427.0, 58.6, 0, 0),
        ("Lava_World_1", 2, 445, 1.2, 21.4, 800.0, 12.3, 0, 0),
        # PlanÃ¨tes trop froides
        ("Ice_Giant_1", 25, 234, 3.8, 2.1, -180.0, 4500.0, 1, 0),
        ("Frozen_Planet_1", 4, 123, 1.1, 3.2, -200.0, 1200.0, 1, 0),
        ("Pluto_Like_1", 5, 67, 0.66, 1.8, -229.0, 6.4, 1, 0),
        ("Europa_Like_1", 0, 89, 1.3, 0.5, -160.0, 3.6, 1, 0),
        ("Titan_Like_1", 0, 78, 1.4, 0.8, -179.0, 15.9, 1, 0),
        # GÃ©antes gazeuses (non habitables)
        ("Gas_Giant_1", 42, 156, 0.6, 22.3, -120.0, 2890.0, 0, 0),
        ("Jupiter_Like_1", 79, 89, 2.36, 20.1, -110.0, 4333.0, 0, 0),
        ("Saturn_Like_1", 83, 123, 0.69, 18.7, -140.0, 10747.0, 0, 0),
        ("Neptune_Like_1", 14, 234, 1.14, 5.4, -200.0, 60182.0, 0, 0),
        ("Uranus_Like_1", 27, 167, 0.89, 4.2, -195.0, 30589.0, 0, 0),
        # PlanÃ¨tes avec conditions extrÃªmes
        ("High_Radiation_1", 2, 345, 1.5, 25.6, 45.0, 89.2, 0, 0),
        ("No_Atmosphere_1", 0, 234, 2.1, 14.8, 200.0, 176.4, 0, 0),
        ("Tidally_Locked_1", 0, 123, 1.2, 24.0, -150.0, 12.4, 0, 0),
        ("Super_Earth_Heavy_1", 8, 567, 4.2, 16.3, 35.0, 234.7, 1, 0),
        ("Mini_Neptune_1", 12, 78, 0.8, 12.8, -45.0, 456.8, 0, 0),
        # PlanÃ¨tes potentiellement habitables mais avec dÃ©fauts
        (
            "Planet_56161",
            3,
            764,
            2.53,
            22.9,
            63.4,
            43.0,
            1,
            0,
        ),  # Trop de minÃ©raux, trop chaud
        ("Heavy_World_1", 6, 234, 3.5, 15.6, 12.0, 67.8, 1, 0),  # GravitÃ© trop forte
        ("Windy_Planet_1", 1, 123, 1.1, 28.9, 18.0, 8.9, 1, 0),  # Trop de soleil
        (
            "Resource_Poor_1",
            0,
            12,
            0.95,
            14.2,
            22.0,
            45.6,
            1,
            0,
        ),  # TrÃ¨s peu de minÃ©raux
        ("Fast_Rotation_1", 2, 89, 1.3, 16.4, 15.0, 2.3, 1, 0),  # Rotation trop rapide
        # PlanÃ¨tes de rÃ©fÃ©rence (Terre-like habitables)
        ("Earth_Like_1", 1, 234, 1.0, 12.0, 15.0, 24.0, 1, 1),
        ("Earth_Like_2", 2, 198, 0.98, 11.8, 18.0, 23.9, 1, 1),
        ("Earth_Like_3", 1, 267, 1.02, 12.3, 12.0, 24.1, 1, 1),
        ("Perfect_World_1", 2, 156, 0.95, 13.2, 22.0, 26.8, 1, 1),
        ("Garden_World_1", 3, 189, 1.05, 11.5, 19.0, 22.4, 1, 1),
    ]

    schema = StructType(
        [
            StructField("Name", StringType(), True),
            StructField("Num_Moons", IntegerType(), True),
            StructField("Minerals", IntegerType(), True),
            StructField("Gravity", DoubleType(), True),
            StructField("Sunlight_Hours", DoubleType(), True),
            StructField("Temperature", DoubleType(), True),
            StructField("Rotation_Time", DoubleType(), True),
            StructField("Water_Presence", IntegerType(), True),
            StructField(
                "Colonisable", IntegerType(), True
            ),  # 1 = habitable, 0 = non habitable
        ]
    )

    df = spark.createDataFrame(enhanced_data, schema)
    return df


def engineer_features(df):
    """
    IngÃ©nierie des features pour amÃ©liorer la prÃ©diction
    """
    # Calcul de nouvelles features
    enhanced_df = (
        df.withColumn(
            "temperature_zone",
            when((col("Temperature") >= -50) & (col("Temperature") <= 50), 1).otherwise(
                0
            ),
        )
        .withColumn(
            "earth_like_gravity",
            when((col("Gravity") >= 0.5) & (col("Gravity") <= 2.0), 1).otherwise(0),
        )
        .withColumn(
            "optimal_sunlight",
            when(
                (col("Sunlight_Hours") >= 8) & (col("Sunlight_Hours") <= 16), 1
            ).otherwise(0),
        )
        .withColumn(
            "moderate_rotation",
            when(
                (col("Rotation_Time") >= 12) & (col("Rotation_Time") <= 48), 1
            ).otherwise(0),
        )
        .withColumn(
            "mineral_richness",
            when(col("Minerals") > 200, 1).otherwise(0),
        )
        .withColumn(
            "moon_stability",
            when((col("Num_Moons") >= 1) & (col("Num_Moons") <= 5), 1).otherwise(0),
        )
        .withColumn(
            "temperature_squared",
            col("Temperature") * col("Temperature"),  # Feature non-linÃ©aire
        )
        .withColumn(
            "gravity_sunlight_interaction",
            col("Gravity") * col("Sunlight_Hours"),  # Interaction entre features
        )
        .withColumn(
            "habitability_score",
            (
                col("temperature_zone")
                + col("earth_like_gravity")
                + col("optimal_sunlight")
                + col("Water_Presence")
                + col("moderate_rotation")
            )
            / 5.0,  # Score d'habitabilitÃ© composite
        )
    )

    return enhanced_df


def prepare_ml_pipeline(df):
    """
    PrÃ©pare le pipeline de machine learning
    """
    # Features numÃ©riques originales
    original_features = [
        "Num_Moons",
        "Minerals",
        "Gravity",
        "Sunlight_Hours",
        "Temperature",
        "Rotation_Time",
        "Water_Presence",
    ]

    # Features engineered
    engineered_features = [
        "temperature_zone",
        "earth_like_gravity",
        "optimal_sunlight",
        "moderate_rotation",
        "mineral_richness",
        "moon_stability",
        "temperature_squared",
        "gravity_sunlight_interaction",
        "habitability_score",
    ]

    # Assemblage de toutes les features
    all_features = original_features + engineered_features
    assembler = VectorAssembler(inputCols=all_features, outputCol="features")

    # Normalisation
    scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")

    return assembler, scaler, all_features


def train_models(df):
    """
    EntraÃ®ne plusieurs modÃ¨les de classification
    """
    print("\nğŸ¤– ENTRAÃNEMENT DES MODÃˆLES D'IA")
    print("=" * 50)

    # IngÃ©nierie des features
    df_features = engineer_features(df)

    # Affichage des statistiques de classe
    print("ğŸ“Š Distribution des classes:")
    df_features.groupBy("Colonisable").count().show()

    print("ğŸ“ˆ Statistiques des features engineered:")
    df_features.select(
        "temperature_zone",
        "earth_like_gravity",
        "optimal_sunlight",
        "moderate_rotation",
        "habitability_score",
    ).describe().show()

    # PrÃ©paration du pipeline
    assembler, scaler, feature_names = prepare_ml_pipeline(df_features)

    # Division train/test
    train_df, test_df = df_features.randomSplit([0.8, 0.2], seed=42)

    print(f"ğŸ¯ DonnÃ©es d'entraÃ®nement: {train_df.count()}")
    print(f"ğŸ§ª DonnÃ©es de test: {test_df.count()}")

    # ModÃ¨les Ã  tester
    models = {
        "RandomForest": RandomForestClassifier(
            featuresCol="scaledFeatures",
            labelCol="Colonisable",
            numTrees=100,
            maxDepth=10,
            seed=42,
        ),
        "GradientBoosting": GBTClassifier(
            featuresCol="scaledFeatures",
            labelCol="Colonisable",
            maxIter=50,
            maxDepth=8,
            seed=42,
        ),
        "LogisticRegression": LogisticRegression(
            featuresCol="scaledFeatures",
            labelCol="Colonisable",
            maxIter=200,
            regParam=0.01,
        ),
    }

    best_model = None
    best_score = 0
    best_name = ""
    results = {}

    # Ã‰valuateurs
    binary_evaluator = BinaryClassificationEvaluator(
        rawPredictionCol="rawPrediction",
        labelCol="Colonisable",
        metricName="areaUnderROC",
    )

    multi_evaluator = MulticlassClassificationEvaluator(
        predictionCol="prediction", labelCol="Colonisable", metricName="accuracy"
    )

    for model_name, model in models.items():
        print(f"\nğŸ”§ EntraÃ®nement du modÃ¨le: {model_name}")

        # Pipeline complet
        pipeline = Pipeline(stages=[assembler, scaler, model])

        # EntraÃ®nement
        model_fitted = pipeline.fit(train_df)

        # PrÃ©dictions sur le test
        predictions = model_fitted.transform(test_df)

        # Ã‰valuation
        auc_score = binary_evaluator.evaluate(predictions)
        accuracy_score = multi_evaluator.evaluate(predictions)

        # Calcul de mÃ©triques supplÃ©mentaires
        f1_evaluator = MulticlassClassificationEvaluator(
            predictionCol="prediction", labelCol="Colonisable", metricName="f1"
        )
        f1_score = f1_evaluator.evaluate(predictions)

        precision_evaluator = MulticlassClassificationEvaluator(
            predictionCol="prediction",
            labelCol="Colonisable",
            metricName="weightedPrecision",
        )
        precision_score = precision_evaluator.evaluate(predictions)

        results[model_name] = {
            "model": model_fitted,
            "auc": auc_score,
            "accuracy": accuracy_score,
            "f1": f1_score,
            "precision": precision_score,
        }

        print(f"  ğŸ“ˆ AUC: {auc_score:.3f}")
        print(f"  ğŸ¯ Accuracy: {accuracy_score:.3f}")
        print(f"  ğŸ“Š F1-Score: {f1_score:.3f}")
        print(f"  ğŸ” Precision: {precision_score:.3f}")

        if auc_score > best_score:
            best_score = auc_score
            best_model = model_fitted
            best_name = model_name

    print(f"\nğŸ† Meilleur modÃ¨le: {best_name} (AUC: {best_score:.3f})")

    return best_model, results, test_df, feature_names


def analyze_feature_importance(model, feature_names):
    """
    Analyse l'importance des features
    """
    print("\nğŸ“Š IMPORTANCE DES FEATURES")
    print("=" * 50)

    try:
        # Extraction du modÃ¨le final (RandomForest ou GBT)
        stages = model.stages
        final_model = stages[-1]

        if hasattr(final_model, "featureImportances"):
            importances = final_model.featureImportances.toArray()

            # CrÃ©ation d'un DataFrame avec les importances
            feature_importance = list(zip(feature_names, importances))
            feature_importance.sort(key=lambda x: x[1], reverse=True)

            print("ğŸ” Top 10 des features les plus importantes:")
            for i, (feature, importance) in enumerate(feature_importance[:10], 1):
                print(f"  {i}. {feature}: {importance:.3f}")

            return feature_importance
        else:
            print("âš ï¸ ModÃ¨le ne supporte pas l'analyse d'importance des features")
            return None

    except Exception as e:
        print(f"âŒ Erreur lors de l'analyse des features: {e}")
        return None


def predict_habitability(model, new_planets_data, spark):
    """
    PrÃ©dit l'habitabilitÃ© de nouvelles planÃ¨tes
    """
    print("\nğŸ”® PRÃ‰DICTION D'HABITABILITÃ‰")
    print("=" * 50)

    # CrÃ©ation du DataFrame pour les nouvelles planÃ¨tes
    schema = StructType(
        [
            StructField("Name", StringType(), True),
            StructField("Num_Moons", IntegerType(), True),
            StructField("Minerals", IntegerType(), True),
            StructField("Gravity", DoubleType(), True),
            StructField("Sunlight_Hours", DoubleType(), True),
            StructField("Temperature", DoubleType(), True),
            StructField("Rotation_Time", DoubleType(), True),
            StructField("Water_Presence", IntegerType(), True),
        ]
    )

    new_df = spark.createDataFrame(new_planets_data, schema)

    # Application de l'ingÃ©nierie des features
    new_df_features = engineer_features(new_df)

    # PrÃ©dictions
    predictions = model.transform(new_df_features)

    # Affichage des rÃ©sultats
    print("ğŸŒ PrÃ©dictions d'habitabilitÃ©:")
    predictions.select(
        "Name",
        "Temperature",
        "Gravity",
        "Water_Presence",
        "habitability_score",
        "prediction",
        "probability",
    ).show(truncate=False)

    # Analyse des prÃ©dictions
    habitable_count = predictions.filter(col("prediction") == 1).count()
    total_count = predictions.count()

    print(f"\nğŸ“Š RÃ©sumÃ© des prÃ©dictions:")
    print(f"  ğŸŒ PlanÃ¨tes prÃ©dites habitables: {habitable_count}/{total_count}")
    print(f"  ğŸ“ˆ Pourcentage d'habitabilitÃ©: {(habitable_count/total_count)*100:.1f}%")

    return predictions


def evaluate_model_robustness(model, df, spark):
    """
    Ã‰value la robustesse du modÃ¨le avec validation croisÃ©e
    """
    print("\nğŸ”„ Ã‰VALUATION DE LA ROBUSTESSE DU MODÃˆLE")
    print("=" * 50)

    try:
        # PrÃ©paration des donnÃ©es
        df_features = engineer_features(df)
        assembler, scaler, _ = prepare_ml_pipeline(df_features)

        # CrÃ©ation d'un modÃ¨le RandomForest pour la validation croisÃ©e
        rf = RandomForestClassifier(
            featuresCol="scaledFeatures", labelCol="Colonisable", numTrees=50, seed=42
        )

        pipeline = Pipeline(stages=[assembler, scaler, rf])

        # Grille de paramÃ¨tres
        paramGrid = (
            ParamGridBuilder()
            .addGrid(rf.numTrees, [30, 50, 100])
            .addGrid(rf.maxDepth, [5, 8, 10])
            .build()
        )

        # Validation croisÃ©e
        evaluator = BinaryClassificationEvaluator(
            labelCol="Colonisable", metricName="areaUnderROC"
        )

        cv = CrossValidator(
            estimator=pipeline,
            estimatorParamMaps=paramGrid,
            evaluator=evaluator,
            numFolds=3,
            seed=42,
        )

        # EntraÃ®nement avec validation croisÃ©e
        cv_model = cv.fit(df_features)

        # Meilleurs paramÃ¨tres
        best_params = cv_model.bestModel.stages[-1].extractParamMap()
        print("ğŸ† Meilleurs paramÃ¨tres trouvÃ©s:")
        for param, value in best_params.items():
            print(f"  {param.name}: {value}")

        # Score de validation croisÃ©e
        avg_score = max(cv_model.avgMetrics)
        print(f"\nğŸ“Š Score moyen de validation croisÃ©e: {avg_score:.3f}")

        return cv_model

    except Exception as e:
        print(f"âŒ Erreur lors de la validation croisÃ©e: {e}")
        return None


def save_model_and_results(model, results, hdfs_namenode):
    """
    Sauvegarde le modÃ¨le et les rÃ©sultats
    """
    print("\nğŸ’¾ SAUVEGARDE DU MODÃˆLE")
    print("=" * 50)

    try:
        # Sauvegarde du modÃ¨le
        model_path = f"{hdfs_namenode}/planet_ml_models/habitability_model"
        model.write().overwrite().save(model_path)
        print(f"âœ… ModÃ¨le sauvegardÃ©: {model_path}")

        # Sauvegarde des mÃ©triques (dans un fichier local pour cet exemple)
        with open("/tmp/model_results.json", "w") as f:
            import json

            metrics = {
                name: {
                    "auc": result["auc"],
                    "accuracy": result["accuracy"],
                    "f1": result["f1"],
                    "precision": result["precision"],
                }
                for name, result in results.items()
            }
            json.dump(metrics, f, indent=2)
        print("âœ… MÃ©triques sauvegardÃ©es: /tmp/model_results.json")

    except Exception as e:
        print(f"âŒ Erreur lors de la sauvegarde: {e}")


def main():
    """
    Fonction principale du modÃ¨le d'habitabilitÃ©
    """
    print("ğŸ¤– MODÃˆLE D'IA: PRÃ‰DICTION D'HABITABILITÃ‰ DES PLANÃˆTES")
    print("=" * 60)

    # Configuration
    hdfs_namenode = os.getenv("HDFS_NAMENODE", "hdfs://namenode:9000")

    # CrÃ©ation de la session Spark
    spark = create_spark_session()

    try:
        # 1. CrÃ©ation du dataset d'entraÃ®nement
        df = create_enhanced_dataset(spark)
        print(f"\nğŸ“Š Dataset crÃ©Ã© avec {df.count()} planÃ¨tes")

        # AperÃ§u du dataset
        print("\nğŸ” AperÃ§u du dataset:")
        df.show(5)

        # 2. EntraÃ®nement des modÃ¨les
        best_model, results, test_df, feature_names = train_models(df)

        # 3. Analyse de l'importance des features
        feature_importance = analyze_feature_importance(best_model, feature_names)

        # 4. Ã‰valuation de la robustesse
        cv_model = evaluate_model_robustness(best_model, df, spark)

        # 5. Exemples de prÃ©diction sur de nouvelles planÃ¨tes
        new_planets = [
            ("Candidate_A", 1, 150, 1.2, 12.0, 18.0, 24.5, 1),  # TrÃ¨s prometteur
            ("Candidate_B", 0, 80, 0.9, 10.5, -15.0, 28.3, 0),  # Moyennement prometteur
            ("Candidate_C", 15, 30, 0.6, 22.0, -120.0, 2500.0, 0),  # GÃ©ante gazeuse
            ("Candidate_D", 2, 250, 1.8, 14.8, 65.0, 45.2, 0),  # Trop chaud
            ("Candidate_E", 3, 189, 1.05, 11.5, 19.0, 22.4, 1),  # TrÃ¨s prometteur
            ("Hot_Desert", 0, 890, 2.1, 18.5, 150.0, 45.6, 0),  # DÃ©sert chaud
        ]

        predictions = predict_habitability(best_model, new_planets, spark)

        # 6. Sauvegarde
        save_model_and_results(best_model, results, hdfs_namenode)

        print("\nâœ… MODÃˆLE D'HABITABILITÃ‰ COMPLÃ‰TÃ‰")
        print(
            "ğŸ¯ Le modÃ¨le peut maintenant prÃ©dire l'habitabilitÃ© de nouvelles planÃ¨tes!"
        )
        print("\nğŸ“‹ Facteurs clÃ©s identifiÃ©s pour l'habitabilitÃ©:")
        print("  â€¢ TempÃ©rature dans la zone habitable (-50Â°C Ã  50Â°C)")
        print("  â€¢ GravitÃ© similaire Ã  la Terre (0.5 Ã  2.0)")
        print("  â€¢ PrÃ©sence d'eau")
        print("  â€¢ Heures de soleil modÃ©rÃ©es (8 Ã  16h)")
        print("  â€¢ Rotation modÃ©rÃ©e (12 Ã  48h)")

    except Exception as e:
        print(f"âŒ Erreur lors de l'entraÃ®nement du modÃ¨le: {e}")
        import traceback

        traceback.print_exc()
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
