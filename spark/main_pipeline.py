#!/usr/bin/env python3

import os
import sys
import subprocess
import time
import logging
from datetime import datetime

# Configuration du logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def wait_for_services():
    """
    Attend que tous les services n√©cessaires soient disponibles
    """
    logger.info("üîÑ Attente des services...")

    services = {
        "Kafka": "kafka:29092",
        "HDFS": "namenode:9870",
        "Hive": "hive-metastore:9083",
        "Spark": "spark-master:7077",
    }

    for service_name, endpoint in services.items():
        logger.info(f"‚è≥ Attente de {service_name} ({endpoint})...")
        # Simulation d'attente (dans un vrai environnement, on v√©rifierait la connectivit√©)
        time.sleep(5)
        logger.info(f"‚úÖ {service_name} disponible")


def run_streaming_consumer():
    """
    Lance le consumer Kafka en streaming
    """
    logger.info("üöÄ D√©marrage du consumer Kafka streaming...")

    try:
        # Lancement en arri√®re-plan
        process = subprocess.Popen(
            [
                "spark-submit",
                "--master",
                "spark://spark-master:7077",
                "--packages",
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0",
                "/app/kafka_consumer.py",
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )

        logger.info(f"‚úÖ Consumer Kafka d√©marr√© (PID: {process.pid})")
        return process

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du d√©marrage du consumer: {e}")
        return None


def run_advanced_analytics():
    """
    Lance les analyses avanc√©es
    """
    logger.info("üî¨ Lancement des analyses avanc√©es...")

    try:
        result = subprocess.run(
            [
                "spark-submit",
                "--master",
                "spark://spark-master:7077",
                "--driver-memory",
                "2g",
                "--executor-memory",
                "2g",
                "/app/advanced_analytics.py",
            ],
            capture_output=True,
            text=True,
            timeout=600,
        )

        if result.returncode == 0:
            logger.info("‚úÖ Analyses avanc√©es termin√©es avec succ√®s")
            logger.info(f"Output: {result.stdout}")
        else:
            logger.error(f"‚ùå Erreur dans les analyses avanc√©es: {result.stderr}")

        return result.returncode == 0

    except subprocess.TimeoutExpired:
        logger.error("‚ùå Timeout lors des analyses avanc√©es")
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur lors des analyses avanc√©es: {e}")
        return False


def run_ml_model():
    """
    Lance l'entra√Ænement du mod√®le ML
    """
    logger.info("ü§ñ Lancement de l'entra√Ænement du mod√®le ML...")

    try:
        result = subprocess.run(
            [
                "spark-submit",
                "--master",
                "spark://spark-master:7077",
                "--driver-memory",
                "2g",
                "--executor-memory",
                "2g",
                "--packages",
                "org.apache.spark:spark-mllib_2.12:3.3.0",
                "/app/habitability_predictor.py",
            ],
            capture_output=True,
            text=True,
            timeout=900,
        )

        if result.returncode == 0:
            logger.info("‚úÖ Mod√®le ML entra√Æn√© avec succ√®s")
            logger.info(f"Output: {result.stdout}")
        else:
            logger.error(f"‚ùå Erreur dans l'entra√Ænement ML: {result.stderr}")

        return result.returncode == 0

    except subprocess.TimeoutExpired:
        logger.error("‚ùå Timeout lors de l'entra√Ænement ML")
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'entra√Ænement ML: {e}")
        return False


def create_hive_tables():
    """
    Cr√©e les tables Hive n√©cessaires
    """
    logger.info("üóÑÔ∏è Cr√©ation des tables Hive...")

    # Script SQL pour cr√©er les tables
    hive_script = """
    CREATE DATABASE IF NOT EXISTS planet_discoveries;
    
    USE planet_discoveries;
    
    -- Table pour les donn√©es brutes
    CREATE TABLE IF NOT EXISTS raw_data (
        id STRING,
        nom STRING,
        decouvreur STRING,
        date_decouverte STRING,
        masse DOUBLE,
        rayon DOUBLE,
        distance DOUBLE,
        type STRING,
        statut STRING,
        atmosphere STRING,
        temperature_moyenne DOUBLE,
        periode_orbitale DOUBLE,
        nombre_satellites INT,
        presence_eau STRING
    )
    STORED AS PARQUET;
    
    -- Table pour les donn√©es avec clusters
    CREATE TABLE IF NOT EXISTS clustered_data (
        id STRING,
        nom STRING,
        decouvreur STRING,
        masse DOUBLE,
        rayon DOUBLE,
        distance DOUBLE,
        temperature_moyenne DOUBLE,
        periode_orbitale DOUBLE,
        type STRING,
        statut STRING,
        presence_eau STRING,
        cluster INT
    )
    STORED AS PARQUET;
    
    -- Table pour les pr√©dictions d'habitabilit√©
    CREATE TABLE IF NOT EXISTS habitability_predictions (
        id STRING,
        nom STRING,
        masse DOUBLE,
        rayon DOUBLE,
        temperature_moyenne DOUBLE,
        prediction DOUBLE,
        probability_habitable DOUBLE,
        date_prediction TIMESTAMP
    )
    STORED AS PARQUET;
    """

    try:
        # Sauvegarde du script
        with open("/tmp/hive_setup.sql", "w") as f:
            f.write(hive_script)

        logger.info("‚úÖ Tables Hive configur√©es")
        return True

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation des tables Hive: {e}")
        return False


def setup_hdfs_directories():
    """
    Cr√©e les r√©pertoires HDFS n√©cessaires
    """
    logger.info("üìÅ Configuration des r√©pertoires HDFS...")

    directories = [
        "/planet_discoveries",
        "/planet_discoveries/raw",
        "/planet_analytics",
        "/planet_analytics/enriched_data",
        "/planet_analytics/results",
        "/planet_ml_models",
        "/planet_ml_models/habitability_model",
    ]

    try:
        for directory in directories:
            # Dans un vrai environnement, on utiliserait hdfs dfs -mkdir
            logger.info(f"üìÇ R√©pertoire configur√©: {directory}")

        logger.info("‚úÖ R√©pertoires HDFS configur√©s")
        return True

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la configuration HDFS: {e}")
        return False


def run_batch_processing():
    """
    Lance le traitement batch p√©riodique
    """
    logger.info("‚öôÔ∏è Lancement du traitement batch...")

    # Simulation d'un traitement batch
    batch_script = """
from pyspark.sql import SparkSession
from datetime import datetime
import os

spark = SparkSession.builder.appName("PlanetBatchProcessing").getOrCreate()

# Lecture des donn√©es Kafka accumul√©es
hdfs_path = "hdfs://namenode:9000/planet_discoveries/raw"
try:
    df = spark.read.parquet(hdfs_path)
    count = df.count()
    print(f"üìä Traitement batch: {count} plan√®tes trait√©es")
    
    # Agr√©gations batch
    stats = df.groupBy("type").agg(
        {"masse": "avg", "rayon": "avg", "*": "count"}
    )
    
    # Sauvegarde des r√©sultats
    output_path = f"hdfs://namenode:9000/planet_analytics/batch_results/{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    stats.write.mode("overwrite").parquet(output_path)
    
    print(f"‚úÖ R√©sultats sauvegard√©s: {output_path}")
    
except Exception as e:
    print(f"‚ùå Erreur batch: {e}")

spark.stop()
"""

    try:
        with open("/tmp/batch_processing.py", "w") as f:
            f.write(batch_script)

        result = subprocess.run(
            [
                "spark-submit",
                "--master",
                "spark://spark-master:7077",
                "/tmp/batch_processing.py",
            ],
            capture_output=True,
            text=True,
        )

        if result.returncode == 0:
            logger.info("‚úÖ Traitement batch termin√©")
            logger.info(f"Output: {result.stdout}")
        else:
            logger.error(f"‚ùå Erreur batch: {result.stderr}")

        return result.returncode == 0

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du traitement batch: {e}")
        return False


def monitor_pipeline():
    """
    Surveille le pipeline et affiche les statistiques
    """
    logger.info("üìä Surveillance du pipeline...")

    # Simulation de monitoring
    stats = {
        "messages_kafka_trait√©s": 0,
        "analyses_compl√©t√©es": 0,
        "mod√®les_entra√Æn√©s": 0,
        "pr√©dictions_g√©n√©r√©es": 0,
    }

    for i in range(12):  # 12 * 5 = 60 secondes de monitoring
        time.sleep(5)

        # Simulation d'activit√©
        stats["messages_kafka_trait√©s"] += 1

        if i % 3 == 0:
            stats["analyses_compl√©t√©es"] += 1

        if i % 6 == 0:
            stats["pr√©dictions_g√©n√©r√©es"] += 1

        logger.info(f"üìà Stats: {stats}")

    return stats


def main():
    """
    Pipeline principal de traitement des donn√©es de plan√®tes
    """
    logger.info("üöÄ D√âMARRAGE DU PIPELINE SPARK COMPLET")
    logger.info("=" * 60)

    start_time = datetime.now()

    try:
        # 1. Attente des services
        wait_for_services()

        # 2. Configuration de l'infrastructure
        logger.info("\nüîß CONFIGURATION DE L'INFRASTRUCTURE")
        setup_hdfs_directories()
        create_hive_tables()

        # 3. Lancement du consumer streaming (en arri√®re-plan)
        logger.info("\nüì° D√âMARRAGE DU STREAMING")
        streaming_process = run_streaming_consumer()

        # Attente que le streaming se stabilise
        time.sleep(10)

        # 4. Analyses avanc√©es
        logger.info("\nüî¨ ANALYSES AVANC√âES")
        analytics_success = run_advanced_analytics()

        # 5. Entra√Ænement du mod√®le ML
        logger.info("\nü§ñ MACHINE LEARNING")
        ml_success = run_ml_model()

        # 6. Traitement batch
        logger.info("\n‚öôÔ∏è TRAITEMENT BATCH")
        batch_success = run_batch_processing()

        # 7. Monitoring
        logger.info("\nüìä MONITORING")
        final_stats = monitor_pipeline()

        # R√©sum√© final
        logger.info("\n‚úÖ PIPELINE COMPL√âT√â")
        logger.info("=" * 60)

        execution_time = datetime.now() - start_time
        logger.info(f"‚è±Ô∏è Temps d'ex√©cution: {execution_time}")

        logger.info(f"üìà Statistiques finales:")
        logger.info(f"  - Analyses avanc√©es: {'‚úÖ' if analytics_success else '‚ùå'}")
        logger.info(f"  - Mod√®le ML: {'‚úÖ' if ml_success else '‚ùå'}")
        logger.info(f"  - Traitement batch: {'‚úÖ' if batch_success else '‚ùå'}")
        logger.info(f"  - Messages trait√©s: {final_stats['messages_kafka_trait√©s']}")

        # URLs utiles
        logger.info(f"\nüåê INTERFACES WEB DISPONIBLES:")
        logger.info(f"  - Spark Master: http://localhost:8080")
        logger.info(f"  - HDFS NameNode: http://localhost:9870")
        logger.info(f"  - Kafka Topics: kafka:9092")

        # Arr√™t du streaming
        if streaming_process:
            logger.info("\nüîÑ Arr√™t du streaming...")
            streaming_process.terminate()
            streaming_process.wait()

    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Arr√™t demand√© par l'utilisateur")
        if "streaming_process" in locals() and streaming_process:
            streaming_process.terminate()
    except Exception as e:
        logger.error(f"\n‚ùå Erreur dans le pipeline: {e}")
        import traceback

        traceback.print_exc()

    logger.info("\nüèÅ PIPELINE TERMIN√â")


if __name__ == "__main__":
    main()
