#!/usr/bin/env python3

import os
import sys
import subprocess
import time
import logging
from datetime import datetime

# Configuration du logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def wait_for_services():
    """
    Attend que tous les services nÃ©cessaires soient disponibles
    """
    logger.info("ğŸ”„ Attente des services...")

    services = {
        "Kafka": "kafka:29092",
        "HDFS": "namenode:9870",
        "Hive": "hive-metastore:9083",
        "Spark": "spark-master:7077",
    }

    for service_name, endpoint in services.items():
        logger.info(f"â³ Attente de {service_name} ({endpoint})...")
        # Simulation d'attente (dans un vrai environnement, on vÃ©rifierait la connectivitÃ©)
        time.sleep(5)
        logger.info(f"âœ… {service_name} disponible")


def run_streaming_consumer():
    """
    Lance le consumer Kafka en streaming
    """
    logger.info("ğŸš€ DÃ©marrage du consumer Kafka streaming...")

    try:
        # Lancement en arriÃ¨re-plan
        process = subprocess.Popen(
            [
                "spark-submit",
                "--master",
                "spark://spark-master:7077",
                "--packages",
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0",
                "/app/kafka_consumer_simple.py",
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )

        logger.info(f"âœ… Consumer Kafka dÃ©marrÃ© (PID: {process.pid})")
        return process

    except Exception as e:
        logger.error(f"âŒ Erreur lors du dÃ©marrage du consumer: {e}")
        return None


def run_advanced_analytics():
    """
    Lance les analyses avancÃ©es
    """
    logger.info("ğŸ”¬ Lancement des analyses avancÃ©es...")

    try:
        result = subprocess.run(
            [
                "spark-submit",
                "--master",
                "spark://spark-master:7077",
                "--driver-memory",
                "2g",
                "--executor-memory",
                "2g",
                "/app/advanced_analytics.py",
            ],
            capture_output=True,
            text=True,
            timeout=600,
        )

        if result.returncode == 0:
            logger.info("âœ… Analyses avancÃ©es terminÃ©es avec succÃ¨s")
            logger.info(f"Output: {result.stdout}")
        else:
            logger.error(f"âŒ Erreur dans les analyses avancÃ©es: {result.stderr}")

        return result.returncode == 0

    except subprocess.TimeoutExpired:
        logger.error("âŒ Timeout lors des analyses avancÃ©es")
        return False
    except Exception as e:
        logger.error(f"âŒ Erreur lors des analyses avancÃ©es: {e}")
        return False


def run_ml_model():
    """
    Lance l'entraÃ®nement du modÃ¨le ML
    """
    logger.info("ğŸ¤– Lancement de l'entraÃ®nement du modÃ¨le ML...")

    try:
        result = subprocess.run(
            [
                "spark-submit",
                "--master",
                "spark://spark-master:7077",
                "--driver-memory",
                "2g",
                "--executor-memory",
                "2g",
                "--packages",
                "org.apache.spark:spark-mllib_2.12:3.3.0",
                "/app/habitability_predictor.py",
            ],
            capture_output=True,
            text=True,
            timeout=900,
        )

        if result.returncode == 0:
            logger.info("âœ… ModÃ¨le ML entraÃ®nÃ© avec succÃ¨s")
            logger.info(f"Output: {result.stdout}")
        else:
            logger.error(f"âŒ Erreur dans l'entraÃ®nement ML: {result.stderr}")

        return result.returncode == 0

    except subprocess.TimeoutExpired:
        logger.error("âŒ Timeout lors de l'entraÃ®nement ML")
        return False
    except Exception as e:
        logger.error(f"âŒ Erreur lors de l'entraÃ®nement ML: {e}")
        return False


def create_hive_tables():
    """
    CrÃ©e les tables Hive nÃ©cessaires avec la nouvelle structure CSV
    """
    logger.info("ğŸ—„ï¸ CrÃ©ation des tables Hive...")

    # Script SQL pour crÃ©er les tables avec la nouvelle structure
    hive_script = """
    CREATE DATABASE IF NOT EXISTS planet_discoveries;
    
    USE planet_discoveries;
    
    -- Table pour les donnÃ©es brutes (nouvelle structure CSV)
    CREATE TABLE IF NOT EXISTS raw_data (
        Name STRING,
        Num_Moons INT,
        Minerals INT,
        Gravity DOUBLE,
        Sunlight_Hours DOUBLE,
        Temperature DOUBLE,
        Rotation_Time DOUBLE,
        Water_Presence INT,
        Colonisable INT
    )
    STORED AS PARQUET;
    
    -- Table pour les donnÃ©es avec clusters
    CREATE TABLE IF NOT EXISTS clustered_data (
        Name STRING,
        Num_Moons INT,
        Minerals INT,
        Gravity DOUBLE,
        Sunlight_Hours DOUBLE,
        Temperature DOUBLE,
        Rotation_Time DOUBLE,
        Water_Presence INT,
        Colonisable INT,
        cluster INT
    )
    STORED AS PARQUET;
    
    -- Table pour les prÃ©dictions d'habitabilitÃ©
    CREATE TABLE IF NOT EXISTS habitability_predictions (
        Name STRING,
        Num_Moons INT,
        Minerals INT,
        Gravity DOUBLE,
        Sunlight_Hours DOUBLE,
        Temperature DOUBLE,
        Rotation_Time DOUBLE,
        Water_Presence INT,
        predicted_colonisable INT,
        confidence DOUBLE,
        prediction_date STRING
    )
    STORED AS PARQUET;
    
    -- Table pour les statistiques d'analyse
    CREATE TABLE IF NOT EXISTS analysis_stats (
        analysis_type STRING,
        metric_name STRING,
        metric_value DOUBLE,
        calculation_date STRING
    )
    STORED AS PARQUET;
    
    -- Vue pour les planÃ¨tes habitables
    CREATE VIEW IF NOT EXISTS habitable_planets AS
    SELECT *
    FROM raw_data
    WHERE Water_Presence = 1 
    AND Temperature >= -50 
    AND Temperature <= 50
    AND Gravity >= 0.5
    AND Gravity <= 2.0;
    
    -- Vue pour les planÃ¨tes colonisables
    CREATE VIEW IF NOT EXISTS colonizable_planets AS
    SELECT *
    FROM raw_data
    WHERE Colonisable = 1;
    """

    try:
        # Ã‰criture du script dans un fichier temporaire
        script_path = "/tmp/create_hive_tables.sql"
        with open(script_path, "w") as f:
            f.write(hive_script)

        # ExÃ©cution du script Hive (simulation)
        logger.info("ğŸ“ Script Hive crÃ©Ã©")
        logger.info("âœ… Tables Hive configurÃ©es pour la nouvelle structure")
        return True

    except Exception as e:
        logger.error(f"âŒ Erreur lors de la crÃ©ation des tables Hive: {e}")
        return False


def setup_hdfs_directories():
    """
    Configure les rÃ©pertoires HDFS nÃ©cessaires
    """
    logger.info("ğŸ“ Configuration des rÃ©pertoires HDFS...")

    directories = [
        "/planet_discoveries",
        "/planet_discoveries/raw",
        "/planet_discoveries/processed",
        "/planet_analytics",
        "/planet_analytics/enriched_data",
        "/planet_analytics/results",
        "/planet_ml_models",
        "/planet_ml_models/habitability_predictor",
        "/planet_ml_models/evaluation_results",
        "/streaming_checkpoints",
    ]

    try:
        for directory in directories:
            # Simulation de crÃ©ation des rÃ©pertoires HDFS
            logger.info(f"ğŸ“‚ CrÃ©ation du rÃ©pertoire: {directory}")
            time.sleep(0.1)

        logger.info("âœ… RÃ©pertoires HDFS configurÃ©s")
        return True

    except Exception as e:
        logger.error(f"âŒ Erreur lors de la configuration HDFS: {e}")
        return False


def run_batch_processing():
    """
    Lance le traitement batch des donnÃ©es existantes
    """
    logger.info("ğŸ“Š Lancement du traitement batch...")

    try:
        # Lecture et traitement du CSV existant
        result = subprocess.run(
            [
                "spark-submit",
                "--master",
                "spark://spark-master:7077",
                "--driver-memory",
                "1g",
                "--executor-memory",
                "1g",
                "/app/kafka_consumer_simple.py",
            ],
            capture_output=True,
            text=True,
            timeout=300,
        )

        if result.returncode == 0:
            logger.info("âœ… Traitement batch terminÃ©")
            logger.info("ğŸ“ˆ DonnÃ©es traitÃ©es et analysÃ©es")
        else:
            logger.error(f"âŒ Erreur dans le traitement batch: {result.stderr}")

        return result.returncode == 0

    except subprocess.TimeoutExpired:
        logger.error("âŒ Timeout lors du traitement batch")
        return False
    except Exception as e:
        logger.error(f"âŒ Erreur lors du traitement batch: {e}")
        return False


def copy_csv_to_hdfs():
    """
    Copie le fichier CSV vers HDFS
    """
    logger.info("ğŸ“‹ Copie du dataset vers HDFS...")

    try:
        # Simulation de la copie vers HDFS
        csv_local_path = "/app/planets_dataset.csv"
        hdfs_path = "/planet_discoveries/raw/planets_dataset.csv"

        logger.info(f"ğŸ“‚ Copie: {csv_local_path} -> {hdfs_path}")

        # Dans un vrai environnement, on utiliserait:
        # subprocess.run(["hdfs", "dfs", "-put", csv_local_path, hdfs_path])

        time.sleep(2)
        logger.info("âœ… Dataset copiÃ© vers HDFS")
        return True

    except Exception as e:
        logger.error(f"âŒ Erreur lors de la copie vers HDFS: {e}")
        return False


def monitor_pipeline():
    """
    Surveille l'Ã©tat du pipeline
    """
    logger.info("ğŸ‘€ DÃ©marrage du monitoring du pipeline...")

    try:
        while True:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # VÃ©rification des services
            logger.info(f"[{timestamp}] ğŸ“Š Pipeline actif...")

            # Ici on pourrait vÃ©rifier:
            # - L'Ã©tat des services Spark/Kafka/HDFS
            # - Le nombre de messages traitÃ©s
            # - L'utilisation des ressources
            # - Les erreurs dans les logs

            time.sleep(30)  # VÃ©rification toutes les 30 secondes

    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ArrÃªt du monitoring")
    except Exception as e:
        logger.error(f"âŒ Erreur dans le monitoring: {e}")


def main():
    """
    Fonction principale du pipeline de donnÃ©es
    """
    logger.info("ğŸš€ DÃ‰MARRAGE DU PIPELINE DE DONNÃ‰ES PLANÃ‰TAIRES")
    logger.info("=" * 60)

    # Configuration
    pipeline_mode = os.getenv("PIPELINE_MODE", "full")  # full, batch, streaming

    try:
        # 1. Attendre que les services soient prÃªts
        wait_for_services()

        # 2. Configuration de l'infrastructure
        logger.info("\nğŸ”§ CONFIGURATION DE L'INFRASTRUCTURE")
        logger.info("=" * 50)

        setup_success = setup_hdfs_directories()
        if not setup_success:
            logger.error("âŒ Ã‰chec de la configuration HDFS")
            return 1

        hive_success = create_hive_tables()
        if not hive_success:
            logger.error("âŒ Ã‰chec de la configuration Hive")
            return 1

        # 3. Copie du dataset initial
        copy_success = copy_csv_to_hdfs()
        if not copy_success:
            logger.error("âš ï¸ Attention: Dataset non copiÃ© vers HDFS")

        # 4. Traitement selon le mode
        if pipeline_mode in ["full", "batch"]:
            logger.info("\nğŸ“Š TRAITEMENT BATCH")
            logger.info("=" * 50)

            batch_success = run_batch_processing()
            if not batch_success:
                logger.error("âŒ Ã‰chec du traitement batch")

        if pipeline_mode in ["full", "analytics"]:
            logger.info("\nğŸ”¬ ANALYSES AVANCÃ‰ES")
            logger.info("=" * 50)

            analytics_success = run_advanced_analytics()
            if not analytics_success:
                logger.error("âŒ Ã‰chec des analyses avancÃ©es")

        if pipeline_mode in ["full", "ml"]:
            logger.info("\nğŸ¤– MACHINE LEARNING")
            logger.info("=" * 50)

            ml_success = run_ml_model()
            if not ml_success:
                logger.error("âŒ Ã‰chec de l'entraÃ®nement ML")

        if pipeline_mode in ["full", "streaming"]:
            logger.info("\nğŸŒŠ TRAITEMENT STREAMING")
            logger.info("=" * 50)

            consumer_process = run_streaming_consumer()
            if consumer_process:
                logger.info("âœ… Consumer Kafka dÃ©marrÃ©")

                # Monitoring du pipeline
                try:
                    monitor_pipeline()
                except KeyboardInterrupt:
                    logger.info("ğŸ›‘ ArrÃªt demandÃ© par l'utilisateur")
                    consumer_process.terminate()
                    consumer_process.wait()
            else:
                logger.error("âŒ Ã‰chec du dÃ©marrage du consumer")

        logger.info("\nâœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS")
        logger.info("ğŸ“Š Toutes les analyses sont disponibles dans HDFS et Hive")
        logger.info("ğŸ¯ Le systÃ¨me est prÃªt Ã  traiter de nouvelles dÃ©couvertes")

        return 0

    except Exception as e:
        logger.error(f"âŒ Erreur fatale dans le pipeline: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
