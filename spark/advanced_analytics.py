from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer
from pyspark.ml.clustering import KMeans
from pyspark.ml.stat import Correlation
from pyspark.ml.evaluation import ClusteringEvaluator
import os
import json


def create_spark_session():
    """
    CrÃ©e une session Spark pour les analyses avancÃ©es
    """
    spark = (
        SparkSession.builder.appName("PlanetAdvancedAnalytics")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.sql.warehouse.dir", "/spark-warehouse")
        .enableHiveSupport()
        .getOrCreate()
    )

    spark.sparkContext.setLogLevel("WARN")
    return spark


def load_planet_data(spark, hdfs_path=None, csv_path=None):
    """
    Charge les donnÃ©es de planÃ¨tes depuis HDFS, CSV ou donnÃ©es de test
    """
    if hdfs_path:
        try:
            df = spark.read.parquet(hdfs_path)
            print(f"âœ… DonnÃ©es chargÃ©es depuis HDFS: {hdfs_path}")
            return df
        except Exception as e:
            print(f"âš ï¸ Erreur HDFS: {e}")

    if csv_path:
        try:
            df = (
                spark.read.option("header", "true")
                .option("inferSchema", "true")
                .csv(csv_path)
            )
            print(f"âœ… DonnÃ©es chargÃ©es depuis CSV: {csv_path}")
            return df
        except Exception as e:
            print(f"âš ï¸ Erreur CSV: {e}")

    # DonnÃ©es de test si aucune source n'est disponible
    test_data = [
        ("Kepler-442b", 2, 45, 0.85, 8.2, -15.5, 112.3, 0, 1),
        ("Kepler-452b", 1, 78, 1.2, 10.5, 5.0, 385.0, 1, 1),
        ("HD-40307g", 3, 62, 1.8, 6.8, 15.0, 197.8, 1, 1),
        ("Proxima-Centauri-b", 0, 34, 1.1, 4.2, -39.0, 11.2, 0, 0),
        ("TRAPPIST-1e", 0, 58, 0.92, 7.1, -22.0, 6.1, 1, 1),
        ("Gliese-667Cc", 1, 71, 1.5, 9.3, -3.0, 28.1, 1, 1),
        ("K2-18b", 2, 83, 2.3, 5.9, -23.0, 33.0, 1, 1),
        ("TOI-715b", 0, 49, 1.55, 8.7, 15.0, 19.3, 0, 0),
        ("LP-890-9c", 1, 38, 1.4, 6.2, -25.0, 8.8, 0, 0),
        ("GJ-357d", 0, 67, 1.7, 4.5, -53.0, 55.7, 0, 0),
    ]

    schema = StructType(
        [
            StructField("Name", StringType(), True),
            StructField("Num_Moons", IntegerType(), True),
            StructField("Minerals", IntegerType(), True),
            StructField("Gravity", DoubleType(), True),
            StructField("Sunlight_Hours", DoubleType(), True),
            StructField("Temperature", DoubleType(), True),
            StructField("Rotation_Time", DoubleType(), True),
            StructField("Water_Presence", IntegerType(), True),
            StructField("Colonisable", IntegerType(), True),
        ]
    )

    df = spark.createDataFrame(test_data, schema)
    print("âœ… DonnÃ©es de test crÃ©Ã©es")
    return df


def calculate_basic_statistics(df):
    """
    Calcule les statistiques de base
    """
    print("\nğŸ“Š STATISTIQUES DE BASE")
    print("=" * 50)

    # Statistiques numÃ©riques
    numeric_cols = [
        "Num_Moons",
        "Minerals",
        "Gravity",
        "Sunlight_Hours",
        "Temperature",
        "Rotation_Time",
    ]
    stats_df = df.select(numeric_cols).describe()
    stats_df.show()

    # Distribution par prÃ©sence d'eau
    print("\nğŸ’§ Distribution par prÃ©sence d'eau:")
    df.groupBy("Water_Presence").count().orderBy(desc("count")).show()

    # Distribution par colonisabilitÃ©
    print("\nğŸ—ï¸ Distribution par colonisabilitÃ©:")
    df.groupBy("Colonisable").count().orderBy(desc("count")).show()

    # Statistiques par nombre de lunes
    print("\nğŸŒ™ Distribution par nombre de lunes:")
    df.groupBy("Num_Moons").count().orderBy("Num_Moons").show()

    return stats_df


def calculate_correlations(df):
    """
    Calcule les corrÃ©lations entre les variables numÃ©riques
    """
    print("\nğŸ”— ANALYSE DES CORRÃ‰LATIONS")
    print("=" * 50)

    # PrÃ©paration des donnÃ©es numÃ©riques
    numeric_cols = [
        "Num_Moons",
        "Minerals",
        "Gravity",
        "Sunlight_Hours",
        "Temperature",
        "Rotation_Time",
    ]

    # CrÃ©ation d'un vecteur de features
    assembler = VectorAssembler(inputCols=numeric_cols, outputCol="features")
    vector_df = assembler.transform(df).select("features")

    # Calcul de la matrice de corrÃ©lation
    correlation_matrix = Correlation.corr(vector_df, "features").head()
    correlation_array = correlation_matrix[0].toArray()

    # Affichage des corrÃ©lations importantes
    print("\nğŸ” CorrÃ©lations significatives (|r| > 0.5):")
    for i, col1 in enumerate(numeric_cols):
        for j, col2 in enumerate(numeric_cols):
            if i < j:  # Ã‰viter les doublons
                corr_value = correlation_array[i][j]
                if abs(corr_value) > 0.5:
                    print(f"  {col1} â†” {col2}: {corr_value:.3f}")

    return correlation_array, numeric_cols


def analyze_habitability_factors(df):
    """
    Analyse spÃ©cifique des facteurs d'habitabilitÃ©
    """
    print("\nğŸŒ ANALYSE: FACTEURS D'HABITABILITÃ‰")
    print("=" * 60)

    # Statistiques par colonisabilitÃ©
    habitability_stats = df.groupBy("Colonisable").agg(
        avg("Temperature").alias("temperature_moyenne"),
        avg("Gravity").alias("gravite_moyenne"),
        avg("Sunlight_Hours").alias("ensoleillement_moyen"),
        avg("Minerals").alias("mineraux_moyens"),
        avg("Water_Presence").alias("pourcentage_eau"),
        count("*").alias("nombre_planetes"),
    )

    print("ğŸ“ˆ Statistiques moyennes par colonisabilitÃ©:")
    habitability_stats.show()

    # Zone de tempÃ©rature habitable (-50 Ã  50Â°C)
    habitable_temp = df.filter((col("Temperature") >= -50) & (col("Temperature") <= 50))

    print(f"\nğŸŒ¡ï¸ PlanÃ¨tes dans la zone de tempÃ©rature habitable (-50Â°C Ã  50Â°C):")
    print(f"Total: {habitable_temp.count()}/{df.count()}")

    habitable_temp.select(
        "Name", "Temperature", "Water_Presence", "Colonisable", "Gravity", "Minerals"
    ).show()

    # PlanÃ¨tes avec eau ET colonisables
    water_and_colonizable = df.filter(
        (col("Water_Presence") == 1) & (col("Colonisable") == 1)
    )

    print(f"\nğŸ’§ğŸ—ï¸ PlanÃ¨tes avec eau ET colonisables:")
    print(f"Total: {water_and_colonizable.count()}/{df.count()}")
    water_and_colonizable.show()

    return habitability_stats


def analyze_resource_potential(df):
    """
    Analyse du potentiel en ressources
    """
    print("\nâ›ï¸ ANALYSE: POTENTIEL EN RESSOURCES")
    print("=" * 60)

    # Classification par richesse minÃ©rale
    df_with_mineral_class = df.withColumn(
        "mineral_class",
        when(col("Minerals") >= 75, "TrÃ¨s riche")
        .when(col("Minerals") >= 50, "Riche")
        .when(col("Minerals") >= 25, "ModÃ©rÃ©")
        .otherwise("Pauvre"),
    )

    print("ğŸ”ï¸ Distribution par richesse minÃ©rale:")
    df_with_mineral_class.groupBy("mineral_class").count().orderBy(desc("count")).show()

    # CorrÃ©lation richesse minÃ©rale vs colonisabilitÃ©
    mineral_colonization = df_with_mineral_class.groupBy(
        "mineral_class", "Colonisable"
    ).count()
    print("â›ï¸ğŸ—ï¸ Richesse minÃ©rale vs ColonisabilitÃ©:")
    mineral_colonization.show()

    # Top planÃ¨tes par ressources
    print("\nğŸ† Top 10 planÃ¨tes par richesse minÃ©rale:")
    df.select(
        "Name", "Minerals", "Water_Presence", "Colonisable", "Temperature"
    ).orderBy(desc("Minerals")).limit(10).show()

    return df_with_mineral_class


def perform_clustering(df):
    """
    Effectue un clustering des planÃ¨tes
    """
    print("\nğŸ¯ CLUSTERING DES PLANÃˆTES")
    print("=" * 50)

    # PrÃ©paration des features pour le clustering
    feature_cols = [
        "Minerals",
        "Gravity",
        "Sunlight_Hours",
        "Temperature",
        "Rotation_Time",
    ]

    # Assemblage des features
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    feature_df = assembler.transform(df)

    # Normalisation des features
    scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")
    scaler_model = scaler.fit(feature_df)
    scaled_df = scaler_model.transform(feature_df)

    # Application du K-Means avec diffÃ©rents nombres de clusters
    best_k = 3
    best_silhouette = -1

    for k in range(2, 6):
        kmeans = KMeans(
            k=k, featuresCol="scaledFeatures", predictionCol="cluster", seed=42
        )
        model = kmeans.fit(scaled_df)
        predictions = model.transform(scaled_df)

        evaluator = ClusteringEvaluator(
            featuresCol="scaledFeatures", predictionCol="cluster"
        )
        silhouette = evaluator.evaluate(predictions)

        print(f"K={k}: Silhouette Score = {silhouette:.3f}")

        if silhouette > best_silhouette:
            best_silhouette = silhouette
            best_k = k

    # ModÃ¨le final avec le meilleur K
    print(
        f"\nğŸ† Meilleur nombre de clusters: {best_k} (Silhouette: {best_silhouette:.3f})"
    )

    kmeans = KMeans(
        k=best_k, featuresCol="scaledFeatures", predictionCol="cluster", seed=42
    )
    model = kmeans.fit(scaled_df)
    clustered_df = model.transform(scaled_df)

    # Analyse des clusters
    print("\nğŸ“Š Analyse des clusters:")
    cluster_stats = (
        clustered_df.groupBy("cluster")
        .agg(
            count("*").alias("nombre_planetes"),
            avg("Minerals").alias("mineraux_moyens"),
            avg("Gravity").alias("gravite_moyenne"),
            avg("Temperature").alias("temperature_moyenne"),
            avg("Sunlight_Hours").alias("ensoleillement_moyen"),
            avg("Water_Presence").alias("pourcentage_eau"),
            avg("Colonisable").alias("pourcentage_colonisable"),
        )
        .orderBy("cluster")
    )

    cluster_stats.show()

    # Affichage des planÃ¨tes par cluster
    for cluster_id in range(best_k):
        print(f"\nğŸŒŒ Cluster {cluster_id}:")
        cluster_planets = clustered_df.filter(col("cluster") == cluster_id).select(
            "Name",
            "Minerals",
            "Gravity",
            "Temperature",
            "Water_Presence",
            "Colonisable",
        )
        cluster_planets.show()

    return clustered_df, model


def identify_anomalies(df):
    """
    Identifie les planÃ¨tes avec des caractÃ©ristiques atypiques
    """
    print("\nğŸš¨ DÃ‰TECTION D'ANOMALIES")
    print("=" * 50)

    # Calcul des quartiles et IQR pour chaque variable numÃ©rique
    numeric_cols = [
        "Minerals",
        "Gravity",
        "Sunlight_Hours",
        "Temperature",
        "Rotation_Time",
    ]

    anomalies = []

    for col_name in numeric_cols:
        # Calcul des quartiles
        quantiles = df.approxQuantile(col_name, [0.25, 0.5, 0.75], 0.05)
        q1, median, q3 = quantiles
        iqr = q3 - q1

        # Seuils d'anomalie
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr

        print(f"\nğŸ“ {col_name}:")
        print(f"  Q1: {q1:.2f}, MÃ©diane: {median:.2f}, Q3: {q3:.2f}")
        print(f"  Seuils d'anomalie: [{lower_bound:.2f}, {upper_bound:.2f}]")

        # Identification des anomalies
        col_anomalies = df.filter(
            (col(col_name) < lower_bound) | (col(col_name) > upper_bound)
        ).select("Name", col_name, "Water_Presence", "Colonisable")

        anomaly_count = col_anomalies.count()
        if anomaly_count > 0:
            print(f"  ğŸ” {anomaly_count} anomalie(s) dÃ©tectÃ©e(s):")
            col_anomalies.show()
            anomalies.extend(col_anomalies.collect())

    return anomalies


def calculate_colonization_score(df):
    """
    Calcule un score de colonisation basÃ© sur multiple facteurs
    """
    print("\nğŸš€ CALCUL DU SCORE DE COLONISATION")
    print("=" * 50)

    # Score basÃ© sur plusieurs facteurs
    df_with_score = df.withColumn(
        "colonization_score",
        (
            # TempÃ©rature idÃ©ale (entre -20 et 30Â°C)
            when((col("Temperature") >= -20) & (col("Temperature") <= 30), 25)
            .when((col("Temperature") >= -50) & (col("Temperature") <= 50), 15)
            .otherwise(0)
            +
            # GravitÃ© proche de la Terre (0.8 Ã  1.2)
            when((col("Gravity") >= 0.8) & (col("Gravity") <= 1.2), 20)
            .when((col("Gravity") >= 0.5) & (col("Gravity") <= 2.0), 10)
            .otherwise(0)
            +
            # PrÃ©sence d'eau
            when(col("Water_Presence") == 1, 30).otherwise(0)
            +
            # Ressources minÃ©rales (normalisÃ© sur 25 points)
            (col("Minerals") * 25 / 100)
        ),
    )

    print("ğŸ† Top 10 planÃ¨tes par score de colonisation:")
    df_with_score.select(
        "Name",
        "colonization_score",
        "Temperature",
        "Gravity",
        "Water_Presence",
        "Minerals",
        "Colonisable",
    ).orderBy(desc("colonization_score")).limit(10).show()

    # Comparaison avec la classification actuelle
    print("\nğŸ“Š Score moyen par classification actuelle:")
    df_with_score.groupBy("Colonisable").agg(
        avg("colonization_score").alias("score_moyen")
    ).show()

    return df_with_score


def save_results_to_hdfs(df, analytics_results, hdfs_namenode):
    """
    Sauvegarde les rÃ©sultats vers HDFS
    """
    print("\nğŸ’¾ SAUVEGARDE VERS HDFS")
    print("=" * 50)

    try:
        # Sauvegarde des donnÃ©es enrichies
        enriched_path = f"{hdfs_namenode}/planet_analytics/enriched_data"
        df.write.mode("overwrite").parquet(enriched_path)
        print(f"âœ… DonnÃ©es enrichies sauvegardÃ©es: {enriched_path}")

        # Sauvegarde des rÃ©sultats d'analyse
        results_path = f"{hdfs_namenode}/planet_analytics/results"

        # Conversion des rÃ©sultats en DataFrame et sauvegarde
        spark = SparkSession.getActiveSession()
        results_df = spark.createDataFrame([analytics_results], ["analysis_results"])
        results_df.write.mode("overwrite").json(results_path)
        print(f"âœ… RÃ©sultats d'analyse sauvegardÃ©s: {results_path}")

    except Exception as e:
        print(f"âŒ Erreur lors de la sauvegarde HDFS: {e}")


def save_to_hive(df, clustered_df):
    """
    Sauvegarde vers Hive
    """
    print("\nğŸ—„ï¸ SAUVEGARDE VERS HIVE")
    print("=" * 50)

    try:
        # Table des donnÃ©es brutes
        df.write.mode("overwrite").saveAsTable("planet_discoveries.raw_data")
        print("âœ… Table 'planet_discoveries.raw_data' crÃ©Ã©e")

        # Table des donnÃ©es avec clusters
        clustered_df.select(
            "Name",
            "Num_Moons",
            "Minerals",
            "Gravity",
            "Sunlight_Hours",
            "Temperature",
            "Rotation_Time",
            "Water_Presence",
            "Colonisable",
            "cluster",
        ).write.mode("overwrite").saveAsTable("planet_discoveries.clustered_data")
        print("âœ… Table 'planet_discoveries.clustered_data' crÃ©Ã©e")

    except Exception as e:
        print(f"âŒ Erreur lors de la sauvegarde Hive: {e}")


def main():
    """
    Fonction principale d'analyse avancÃ©e
    """
    print("ğŸ”¬ ANALYSES AVANCÃ‰ES DES DÃ‰COUVERTES DE PLANÃˆTES")
    print("=" * 60)

    # Configuration
    hdfs_namenode = os.getenv("HDFS_NAMENODE", "hdfs://namenode:9000")
    hdfs_data_path = f"{hdfs_namenode}/planet_discoveries/raw"
    csv_path = "/app/planets_dataset.csv"

    # CrÃ©ation de la session Spark
    spark = create_spark_session()

    try:
        # Chargement des donnÃ©es
        df = load_planet_data(spark, hdfs_data_path, csv_path)

        print(f"\nğŸ“Š Nombre total de planÃ¨tes: {df.count()}")
        print("\nğŸ” AperÃ§u des donnÃ©es:")
        df.show(5)

        # 1. Statistiques de base
        stats_df = calculate_basic_statistics(df)

        # 2. Analyse des corrÃ©lations
        correlation_matrix, numeric_cols = calculate_correlations(df)

        # 3. Analyse des facteurs d'habitabilitÃ©
        habitability_stats = analyze_habitability_factors(df)

        # 4. Analyse du potentiel en ressources
        resource_df = analyze_resource_potential(df)

        # 5. Clustering des planÃ¨tes
        clustered_df, cluster_model = perform_clustering(df)

        # 6. DÃ©tection d'anomalies
        anomalies = identify_anomalies(df)

        # 7. Calcul du score de colonisation
        scored_df = calculate_colonization_score(df)

        # 8. Compilation des rÃ©sultats
        analytics_results = {
            "total_planets": df.count(),
            "correlation_analysis": "completed",
            "clustering_completed": True,
            "anomalies_detected": len(anomalies),
            "water_bearing_planets": df.filter(col("Water_Presence") == 1).count(),
            "colonizable_planets": df.filter(col("Colonisable") == 1).count(),
        }

        # 9. Sauvegarde des rÃ©sultats
        save_results_to_hdfs(scored_df, analytics_results, hdfs_namenode)
        save_to_hive(df, clustered_df)

        print("\nâœ… ANALYSE COMPLÃˆTE TERMINÃ‰E")
        print(f"ğŸ“ˆ RÃ©sultats disponibles dans HDFS: {hdfs_namenode}/planet_analytics/")
        print(
            "ğŸ—„ï¸ Tables Hive crÃ©Ã©es: planet_discoveries.raw_data, planet_discoveries.clustered_data"
        )

    except Exception as e:
        print(f"âŒ Erreur lors de l'analyse: {e}")
        import traceback

        traceback.print_exc()
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
