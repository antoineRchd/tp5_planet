#!/usr/bin/env python3
"""
Processeur Kafka-Spark pour traitement en temps rÃ©el des dÃ©couvertes de planÃ¨tes
Utilise la nouvelle structure de donnÃ©es
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
import os
import json


def create_spark_session():
    """
    CrÃ©e une session Spark avec support Kafka
    """
    spark = (
        SparkSession.builder.appName("PlanetKafkaProcessor")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config(
            "spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0"
        )
        .config("spark.sql.warehouse.dir", "/spark-warehouse")
        .enableHiveSupport()
        .getOrCreate()
    )

    spark.sparkContext.setLogLevel("WARN")
    return spark


def define_planet_schema():
    """
    DÃ©finit le schÃ©ma pour les donnÃ©es de planÃ¨tes depuis Kafka
    """
    return StructType(
        [
            StructField("Name", StringType(), True),
            StructField("Num_Moons", IntegerType(), True),
            StructField("Minerals", IntegerType(), True),
            StructField("Gravity", DoubleType(), True),
            StructField("Sunlight_Hours", DoubleType(), True),
            StructField("Temperature", DoubleType(), True),
            StructField("Rotation_Time", DoubleType(), True),
            StructField("Water_Presence", IntegerType(), True),
            StructField("Colonisable", IntegerType(), True),
            StructField("timestamp", TimestampType(), True),
        ]
    )


def engineer_features_streaming(df):
    """
    Applique l'ingÃ©nierie des features aux donnÃ©es streaming
    """
    return (
        df.withColumn(
            "temperature_zone",
            when((col("Temperature") >= -50) & (col("Temperature") <= 50), 1).otherwise(
                0
            ),
        )
        .withColumn(
            "earth_like_gravity",
            when((col("Gravity") >= 0.5) & (col("Gravity") <= 2.0), 1).otherwise(0),
        )
        .withColumn(
            "optimal_sunlight",
            when(
                (col("Sunlight_Hours") >= 8) & (col("Sunlight_Hours") <= 16), 1
            ).otherwise(0),
        )
        .withColumn(
            "moderate_rotation",
            when(
                (col("Rotation_Time") >= 12) & (col("Rotation_Time") <= 48), 1
            ).otherwise(0),
        )
        .withColumn(
            "mineral_richness",
            when(col("Minerals") > 200, 1).otherwise(0),
        )
        .withColumn(
            "moon_stability",
            when((col("Num_Moons") >= 1) & (col("Num_Moons") <= 5), 1).otherwise(0),
        )
        .withColumn(
            "habitability_score",
            (
                col("temperature_zone")
                + col("earth_like_gravity")
                + col("optimal_sunlight")
                + col("Water_Presence")
                + col("moderate_rotation")
            )
            / 5.0,
        )
        .withColumn("analysis_timestamp", current_timestamp())
    )


def analyze_streaming_data(df):
    """
    Effectue des analyses en temps rÃ©el sur les donnÃ©es streaming
    """
    # Ajout de classifications rapides
    classified_df = (
        df.withColumn(
            "planet_class",
            when(col("habitability_score") >= 0.8, "Highly Habitable")
            .when(col("habitability_score") >= 0.6, "Potentially Habitable")
            .when(col("habitability_score") >= 0.4, "Marginally Habitable")
            .otherwise("Not Habitable"),
        )
        .withColumn(
            "priority",
            when(col("habitability_score") >= 0.8, "HIGH")
            .when(col("habitability_score") >= 0.6, "MEDIUM")
            .when(col("habitability_score") >= 0.4, "LOW")
            .otherwise("NONE"),
        )
        .withColumn(
            "requires_further_study",
            when(
                (col("Water_Presence") == 1) & (col("temperature_zone") == 1), True
            ).otherwise(False),
        )
    )

    return classified_df


def process_kafka_stream():
    """
    Traite le stream Kafka des dÃ©couvertes de planÃ¨tes
    """
    print("ğŸ“¡ DÃ‰MARRAGE DU PROCESSEUR KAFKA-SPARK")
    print("=" * 50)

    # Configuration
    kafka_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")
    hdfs_namenode = os.getenv("HDFS_NAMENODE", "hdfs://namenode:9000")

    print(f"ğŸ”— Kafka Servers: {kafka_servers}")
    print(f"ğŸ’¾ HDFS Namenode: {hdfs_namenode}")

    # CrÃ©ation de la session Spark
    spark = create_spark_session()

    try:
        # Lecture du stream Kafka
        kafka_df = (
            spark.readStream.format("kafka")
            .option("kafka.bootstrap.servers", kafka_servers)
            .option("subscribe", "planet-discoveries")
            .option("startingOffsets", "latest")
            .option("failOnDataLoss", "false")
            .load()
        )

        # SchÃ©ma des donnÃ©es de planÃ¨tes
        planet_schema = define_planet_schema()

        # Parsing des messages JSON
        parsed_df = (
            kafka_df.select(
                from_json(col("value").cast("string"), planet_schema).alias("data")
            )
            .select("data.*")
            .withColumn("processing_time", current_timestamp())
        )

        print("âœ… Stream Kafka configurÃ©")

        # Application de l'ingÃ©nierie des features
        featured_df = engineer_features_streaming(parsed_df)

        # Analyses en temps rÃ©el
        analyzed_df = analyze_streaming_data(featured_df)

        print("âœ… Pipeline d'analyse configurÃ©")

        # Configuration de la sortie streaming vers la console
        console_query = (
            analyzed_df.select(
                "Name",
                "Temperature",
                "Gravity",
                "Water_Presence",
                "habitability_score",
                "planet_class",
                "priority",
                "requires_further_study",
                "processing_time",
            )
            .writeStream.outputMode("append")
            .format("console")
            .option("truncate", "false")
            .option("numRows", 20)
            .trigger(processingTime="10 seconds")
            .start()
        )

        # Configuration de la sortie vers HDFS (checkpoint)
        hdfs_query = (
            analyzed_df.select(
                "Name",
                "Num_Moons",
                "Minerals",
                "Gravity",
                "Sunlight_Hours",
                "Temperature",
                "Rotation_Time",
                "Water_Presence",
                "Colonisable",
                "habitability_score",
                "planet_class",
                "priority",
                "requires_further_study",
                "processing_time",
            )
            .writeStream.outputMode("append")
            .format("parquet")
            .option("path", f"{hdfs_namenode}/streaming/planet_analysis")
            .option("checkpointLocation", f"{hdfs_namenode}/streaming/checkpoints")
            .trigger(processingTime="30 seconds")
            .start()
        )

        print("âœ… Streams de sortie configurÃ©s")
        print("\nğŸ”„ TRAITEMENT EN COURS...")
        print("ğŸ“Š Affichage des rÃ©sultats toutes les 10 secondes")
        print("ğŸ’¾ Sauvegarde HDFS toutes les 30 secondes")
        print("ğŸ›‘ Ctrl+C pour arrÃªter")

        # Attendre les streams
        console_query.awaitTermination()
        hdfs_query.awaitTermination()

    except KeyboardInterrupt:
        print("\nğŸ›‘ ArrÃªt demandÃ© par l'utilisateur")
    except Exception as e:
        print(f"âŒ Erreur lors du traitement streaming: {e}")
        import traceback

        traceback.print_exc()
    finally:
        spark.stop()
        print("âœ… Session Spark fermÃ©e")


def process_batch_analytics():
    """
    Effectue des analyses batch sur les donnÃ©es accumulÃ©es
    """
    print("\nğŸ“Š ANALYSE BATCH DES DONNÃ‰ES ACCUMULÃ‰ES")
    print("=" * 50)

    # Configuration
    hdfs_namenode = os.getenv("HDFS_NAMENODE", "hdfs://namenode:9000")
    spark = create_spark_session()

    try:
        # Lecture des donnÃ©es accumulÃ©es
        streaming_path = f"{hdfs_namenode}/streaming/planet_analysis"

        # VÃ©rifier si des donnÃ©es existent
        try:
            df = spark.read.parquet(streaming_path)
            total_planets = df.count()

            if total_planets == 0:
                print("âš ï¸ Aucune donnÃ©e trouvÃ©e dans le stream")
                return

            print(f"ğŸ“Š Total de planÃ¨tes analysÃ©es: {total_planets}")

            # Analyses des classifications
            print("\nğŸ·ï¸ Distribution des classifications:")
            df.groupBy("planet_class").count().orderBy(desc("count")).show()

            print("\nğŸ¯ Distribution des prioritÃ©s:")
            df.groupBy("priority").count().orderBy(desc("count")).show()

            # PlanÃ¨tes nÃ©cessitant une Ã©tude approfondie
            study_required = df.filter(col("requires_further_study") == True)
            study_count = study_required.count()

            print(f"\nğŸ”¬ PlanÃ¨tes nÃ©cessitant une Ã©tude approfondie: {study_count}")
            if study_count > 0:
                study_required.select(
                    "Name",
                    "Temperature",
                    "Water_Presence",
                    "habitability_score",
                    "planet_class",
                ).show()

            # Statistiques temporelles
            print("\nâ° Analyse temporelle:")
            df.groupBy(
                date_format(col("processing_time"), "yyyy-MM-dd HH").alias("hour")
            ).count().orderBy("hour").show()

            # Top planÃ¨tes habitables
            print("\nğŸŒŸ Top 10 planÃ¨tes les plus habitables:")
            df.filter(col("habitability_score") > 0.5).orderBy(
                desc("habitability_score")
            ).select(
                "Name",
                "habitability_score",
                "planet_class",
                "Temperature",
                "Water_Presence",
            ).limit(
                10
            ).show()

            # Sauvegarde des rÃ©sultats batch
            summary_data = [
                (
                    total_planets,
                    df.filter(col("planet_class") == "Highly Habitable").count(),
                    df.filter(col("requires_further_study") == True).count(),
                    df.agg(avg("habitability_score")).collect()[0][0],
                    current_timestamp(),
                )
            ]

            summary_schema = StructType(
                [
                    StructField("total_planets", LongType(), True),
                    StructField("highly_habitable", LongType(), True),
                    StructField("requires_study", LongType(), True),
                    StructField("avg_habitability", DoubleType(), True),
                    StructField("analysis_time", TimestampType(), True),
                ]
            )

            summary_df = spark.createDataFrame(summary_data, summary_schema)
            summary_df.write.mode("append").parquet(
                f"{hdfs_namenode}/streaming/batch_summaries"
            )

            print("âœ… RÃ©sumÃ© batch sauvegardÃ©")

        except Exception as read_error:
            print(f"âš ï¸ Impossible de lire les donnÃ©es streaming: {read_error}")
            print("   Assurez-vous que le streaming a Ã©tÃ© lancÃ© au prÃ©alable")

    except Exception as e:
        print(f"âŒ Erreur lors de l'analyse batch: {e}")
    finally:
        spark.stop()


def main():
    """
    Fonction principale avec menu de choix
    """
    print("ğŸŒ PROCESSEUR KAFKA-SPARK POUR DÃ‰COUVERTES DE PLANÃˆTES")
    print("=" * 60)

    print("\nOptions disponibles:")
    print("1. ğŸ“¡ Traitement en temps rÃ©el (Streaming)")
    print("2. ğŸ“Š Analyse batch des donnÃ©es accumulÃ©es")
    print("3. ğŸ”„ Les deux (Streaming puis Batch)")

    try:
        choice = input("\nChoisissez une option (1-3): ").strip()

        if choice == "1":
            process_kafka_stream()
        elif choice == "2":
            process_batch_analytics()
        elif choice == "3":
            print("ğŸš€ Lancement du streaming (Ctrl+C pour passer au batch)")
            try:
                process_kafka_stream()
            except KeyboardInterrupt:
                print("\nğŸ”„ Passage Ã  l'analyse batch...")
                process_batch_analytics()
        else:
            print("âŒ Option invalide")

    except KeyboardInterrupt:
        print("\nğŸ›‘ Programme interrompu")
    except Exception as e:
        print(f"âŒ Erreur: {e}")


if __name__ == "__main__":
    main()
