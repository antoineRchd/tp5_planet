#!/usr/bin/env python3
"""
Script simple pour lire les donnÃ©es depuis HDFS
"""

import subprocess
import sys


def read_planet_simple(planet_name):
    """Lire une planÃ¨te avec un script Spark simplifiÃ©"""
    print(f"ðŸ” LECTURE SIMPLE DE '{planet_name}' DEPUIS HDFS")
    print("=" * 60)

    # Script Spark ultra-simplifiÃ©
    spark_script = f"""
from pyspark.sql import SparkSession
import sys

# Configuration Spark minimale
spark = SparkSession.builder \\
    .appName("SimpleRead") \\
    .config("spark.sql.adaptive.enabled", "false") \\
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \\
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

try:
    print("ðŸŒ DONNÃ‰ES BRUTES:")
    raw_data = spark.read.parquet("hdfs://namenode:9000/planet_analytics/raw_data/planets_*")
    planet_data = raw_data.filter(raw_data.Name == "{planet_name}")
    
    count = planet_data.count()
    print(f"Nombre d'enregistrements trouvÃ©s: {{count}}")
    
    if count > 0:
        # Afficher un Ã©chantillon
        print("\\nÃ‰chantillon des donnÃ©es:")
        planet_data.select("Name", "Temperature", "Gravity", "Water_Presence", "Colonisable").limit(3).show()
        
        # Statistiques de base
        planet_sample = planet_data.first()
        print(f"\\nðŸ“‹ CaractÃ©ristiques de {planet_name}:")
        print(f"   â€¢ TempÃ©rature: {{planet_sample['Temperature']}}Â°C")
        print(f"   â€¢ GravitÃ©: {{planet_sample['Gravity']}}G")
        print(f"   â€¢ Eau prÃ©sente: {{'Oui' if planet_sample['Water_Presence'] == 1 else 'Non'}}")
        print(f"   â€¢ Colonisable: {{'Oui' if planet_sample['Colonisable'] == 1 else 'Non'}}")
        print(f"   â€¢ MinÃ©raux: {{planet_sample['Minerals']}} unitÃ©s")
        print(f"   â€¢ Lunes: {{planet_sample['Num_Moons']}}")
        
        # VÃ©rifier les analyses
        try:
            habitability = spark.read.parquet("hdfs://namenode:9000/planet_analytics/results/habitability_*")
            hab_data = habitability.filter(habitability.Name == "{planet_name}")
            if hab_data.count() > 0:
                hab_info = hab_data.first()
                print(f"\\nðŸŒ± HABITABILITÃ‰: {{hab_info['conditions_habitables']}}")
        except:
            print("\\nâš ï¸  Pas d'analyse d'habitabilitÃ© trouvÃ©e")
            
        try:
            colonisation = spark.read.parquet("hdfs://namenode:9000/planet_analytics/results/colonisation_*")
            col_data = colonisation.filter(colonisation.Name == "{planet_name}")
            if col_data.count() > 0:
                col_info = col_data.first()
                print(f"ðŸš€ COLONISATION: Score {{col_info['score_colonisation']}}/100 ({{col_info['potentiel_colonisation']}})")
        except:
            print("âš ï¸  Pas d'analyse de colonisation trouvÃ©e")
    else:
        print(f"âŒ PlanÃ¨te '{planet_name}' non trouvÃ©e dans HDFS")
        
except Exception as e:
    print(f"âŒ Erreur: {{e}}")
    import traceback
    traceback.print_exc()
finally:
    spark.stop()
"""

    try:
        # Ã‰crire le script
        with open("/tmp/simple_read.py", "w") as f:
            f.write(spark_script)

        # Copier dans le container
        subprocess.run(
            [
                "docker",
                "cp",
                "/tmp/simple_read.py",
                "tp5_planet-spark-app:/tmp/simple_read.py",
            ],
            timeout=10,
        )

        print("â³ ExÃ©cution de la lecture HDFS...")

        # ExÃ©cuter avec un timeout plus court et en mode local
        result = subprocess.run(
            [
                "docker",
                "exec",
                "tp5_planet-spark-app",
                "/spark/bin/spark-submit",
                "--master",
                "local[1]",
                "--conf",
                "spark.sql.adaptive.enabled=false",
                "--conf",
                "spark.ui.enabled=false",
                "/tmp/simple_read.py",
            ],
            capture_output=True,
            text=True,
            timeout=90,
        )

        if result.returncode == 0:
            # Filtrer les logs pour ne garder que les infos importantes
            output_lines = result.stdout.split("\n")
            important_lines = []
            capture = False

            for line in output_lines:
                if "ðŸŒ DONNÃ‰ES BRUTES:" in line:
                    capture = True
                elif "INFO" in line and capture:
                    continue
                elif "WARN" in line and capture:
                    continue

                if capture:
                    important_lines.append(line)

            if important_lines:
                print("\n".join(important_lines))
            else:
                print("âœ… Lecture terminÃ©e - voir les logs complets si nÃ©cessaire")
        else:
            print(f"âŒ Erreur d'exÃ©cution: {result.stderr}")

        # Nettoyage
        subprocess.run(["rm", "-f", "/tmp/simple_read.py"])

    except subprocess.TimeoutExpired:
        print("â° Timeout - Le processus prend trop de temps")
        print("ðŸ’¡ Les donnÃ©es sont prÃ©sentes dans HDFS (voir vÃ©rification prÃ©cÃ©dente)")
    except Exception as e:
        print(f"âŒ Erreur: {e}")


if __name__ == "__main__":
    if len(sys.argv) > 1:
        planet_name = sys.argv[1]
        read_planet_simple(planet_name)
    else:
        read_planet_simple("NouvelleTerre")
